<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="http://gfidente.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="http://gfidente.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="http://gfidente.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Giulio Fidente">
  <meta name="description" content="Posts and writings by Giulio Fidente">

  <link href="http://gfidente.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Giulio Fidente Atom" />

<meta name="keywords" content="openstack, ceph, tripleo">

  <title>
    Giulio Fidente
&ndash; Ceph, TripleO and the Newton release  </title>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39216528-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>

<body>
  <aside>
    <div id="user_meta">
      <a href="http://gfidente.com">
        <img src="http://gfidente.com/images/gravatar_to140.png" alt="logo">
      </a>
      <h2><a href="http://gfidente.com">Giulio Fidente</a></h2>
      <p>Software engineer devoted to Automation. Work @Red Hat.</p>
      <ul>
        <li><a href="http://gfidente.com/pages/about.html">About</a></li>
        <li><a href="http://gfidente.com/pages/projects.html">Projects</a></li>
        <li><a href="http://gfidente.com/category/fun.html">fun</a></li>
        <li><a href="http://gfidente.com/category/misc.html">misc</a></li>
        <li><a href="http://gfidente.com/category/politics.html">politics</a></li>
        <li><a href="http://gfidente.com/category/techie.html">techie</a></li>
        <li><a href="mailto:giulioATgiuliofidenteDOTcom" target="_blank">Email</a></li>
      </ul>
    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="http://gfidente.com">Index</a> &brvbar; <a href="http://gfidente.com/archives.html">Archives</a>
      &brvbar; <a href="http://gfidente.com/feeds/all.atom.xml">Atom</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="http://gfidente.com/2016/08/ceph-tripleo-and-the-newton-release.html">Ceph, TripleO and the Newton release</a></h1>
  </div>
  <div class="article_meta">
    <p>Posted on: Fri 26 August 2016</p>
  </div>
  <div class="article_text">
    <p>Time to roll up some notes on the status of Ceph in TripleO. The majority of these functionalities were available in the Mitaka release too but the examples work with code from the Newton release so they might not apply identical to Mitaka.</p>
<div class="section" id="the-tripleo-default-configuration">
<h2>The TripleO default configuration</h2>
<p>No default is going to fit everybody, but we want to know what the default is to improve from there. So let's try and see:</p>
<pre class="literal-block">
uc$ openstack overcloud deploy --templates tripleo-heat-templates -e tripleo-heat-templates/environments/puppet-pacemaker.yaml -e tripleo-heat-templates/environments/storage-environment.yaml --ceph-storage-scale 1
Deploying templates in the directory /home/stack/example/tripleo-heat-templates
...
Overcloud Deployed
</pre>
<p>Monitors go on the <cite>controller</cite> nodes, one per node, the above command is deploying a single controller though. First interesting thing to point out is:</p>
<pre class="literal-block">
oc$ ceph --version
ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)
</pre>
<p>Jewel! Kudos to <a class="reference external" href="http://my1.fr/blog/">Emilien</a> for bringing support for it in <code>puppet-ceph</code>. Continuing our investigation, we notice the OSDs go on the <cite>cephstorage</cite> nodes and are backed by the local filesystem, as we didn't tell it to do differently:</p>
<pre class="literal-block">
oc$ ceph osd tree
ID WEIGHT  TYPE NAME                        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.03999 root default
-2 0.03999     host overcloud-cephstorage-0
 0 0.03999         osd.0                         up  1.00000          1.00000
</pre>
<p>Notice we got SELinux covered:</p>
<pre class="literal-block">
oc$ ls -laZ /srv/data
drwxr-xr-x. ceph ceph system_u:object_r:ceph_var_lib_t:s0 .
...
</pre>
<p>And use CephX with autogenerated keys:</p>
<pre class="literal-block">
oc$ ceph auth list
installed auth entries:

client.admin
        key: AQC2Pr9XAAAAABAAOpviw6DqOMG0syeEYmX2EQ==
        caps: [mds] allow *
        caps: [mon] allow *
        caps: [osd] allow *
client.openstack
        key: AQC2Pr9XAAAAABAAA78Svmmt+LVIcRrZRQLacw==
        caps: [mon] allow r
        caps: [osd] allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=backups, allow rwx pool=vms, allow rwx pool=images, allow rwx pool=metrics
</pre>
<p>But which OpenStack service is using Ceph? The <code>storage-environment.yaml</code> file has some informations:</p>
<pre class="literal-block">
uc$ grep -v '#' tripleo-heat-templates/environments/storage-environment.yaml | uniq

 resource_registry:
   OS::TripleO::Services::CephMon: ../puppet/services/ceph-mon.yaml
   OS::TripleO::Services::CephOSD: ../puppet/services/ceph-osd.yaml
   OS::TripleO::Services::CephClient: ../puppet/services/ceph-client.yaml

 parameter_defaults:
   CinderEnableIscsiBackend: false
   CinderEnableRbdBackend: true
   CinderBackupBackend: ceph
   NovaEnableRbdBackend: true
   GlanceBackend: rbd
   GnocchiBackend: rbd
</pre>
<p>The registry lines enable the Ceph services, the parameters instead are setting Ceph as backend for Cinder, Nova, Glance and Gnocchi. They can be configured to use other backends, see the comments in the environment file. Regarding the pools:</p>
<pre class="literal-block">
oc$ ceph osd lspools
0 rbd,1 metrics,2 images,3 backups,4 volumes,5 vms,
</pre>
<p>Despite the replica size set by default to 3, we only have a single OSD so with a single OSD the cluster will never get into HEALTH_OK:</p>
<pre class="literal-block">
oc$ ceph osd pool get vms size
size: 3
</pre>
<p>Good to know, now a new deployment with more interesting stuff.</p>
</div>
<div class="section" id="a-more-realistic-scenario">
<h2>A more realistic scenario</h2>
<p>What makes it &quot;more realistic&quot;? We'll have enough OSDs to cover the replica size. We'll use physical disks for our OSDs (and journals) and not the local filesystem. We'll cope with a node with a different disks topology and we'll decrease the replica size for one of the pools.</p>
<div class="section" id="set-a-default-disks-map-for-the-osd-nodes">
<h3>Set a default disks map for the OSD nodes</h3>
<p>Define a default configuration for the storage nodes, telling TripleO to use <code>sdb</code> for the OSD data and <code>sdc</code> for the journal:</p>
<pre class="literal-block">
ceph_default_disks.yaml
  parameter_defaults:
    CephStorageExtraConfig:
      ceph::profile::params::osds:
        /dev/sdb:
          journal: /dev/sdc
</pre>
</div>
<div class="section" id="customize-the-disks-map-for-a-specific-node">
<h3>Customize the disks map for a specific node</h3>
<p>For the node which has two (instead of a single) rotatory disks, we'll need a specific map. First get its system-uuid from the Ironic introspection data:</p>
<pre class="literal-block">
uc$ openstack baremetal introspection data save | jq .extra.system.product.uuid
&quot;66C033FA-BAC0-4364-9E8A-3184B5952370&quot;
</pre>
<p>then create the node specific map:</p>
<pre class="literal-block">
ceph_mynode_disks.yaml
  resource_registry:
    OS::TripleO::CephStorageExtraConfigPre: tripleo-heat-templates/puppet/extraconfig/pre_deploy/per_node.yaml

  parameter_defaults:
    NodeDataLookup: &gt;
     {&quot;66C033FA-BAC0-4364-9E8A-3184B5952370&quot;:
       {&quot;ceph::profile::params::osds&quot;:
         {&quot;/dev/sdb&quot;: {&quot;journal&quot;: &quot;/dev/sdd&quot;},
          &quot;/dev/sdc&quot;: {&quot;journal&quot;: &quot;/dev/sdd&quot;}
         }
       }
     }
</pre>
</div>
<div class="section" id="fine-tune-pg-num-pgp-num-and-replica-size-for-a-pool">
<h3>Fine tune pg_num, pgp_num and replica size for a pool</h3>
<p>Finally, to override the replica size (and why not, PGs number) of the &quot;vms&quot; pool (where by default the Nova ephemeral disks go):</p>
<pre class="literal-block">
ceph_pools_config.yaml
  parameter_defaults:
    CephPools:
      vms:
        size: 2
        pg_num: 128
        pgp_num: 128
</pre>
</div>
<div class="section" id="zap-all-disks-for-the-new-deployment">
<h3>Zap all disks for the new deployment</h3>
<p>We also want to clear and prepare all the non-root disks with a GPT label, which will allow us, for example, to repeat the deployment multiple times reusing the same nodes. The implementation of the disks cleanup script can vary, but we can use <a class="reference external" href="https://gist.github.com/gfidente/42d3cdfe0c67f7c95f0c">a sample script</a> and wire it to the overcloud nodes via <cite>NodeUserData</cite>:</p>
<pre class="literal-block">
uc$ curl -O https://gist.githubusercontent.com/gfidente/42d3cdfe0c67f7c95f0c/raw/1f467c6018ada194b54f22113522db61ef944e20/ceph_wipe_disk.yaml

ceph_wipe_env.yaml:
  resource_registry:
    OS::TripleO::NodeUserData: ceph_wipe_disk.yaml

  parameter_defaults:
    ceph_disks: &quot;/dev/sdb /dev/sdc /dev/sdd&quot;
</pre>
<p>All the above environment files could have been merged in a single one but we split them out in multiple ones for clarity. Now the new deploy command:</p>
<pre class="literal-block">
uc$ openstack overcloud deploy --templates tripleo-heat-templates -e tripleo-heat-templates/environments/puppet-pacemaker.yaml -e tripleo-heat-templates/environments/storage-environment.yaml --ceph-storage-scale 3 -e ceph_pools_config.yaml -e ceph_mynode_disks.yaml -e ceph_default_disks.yaml -e ceph_wipe_env.yaml
Deploying templates in the directory /home/stack/example/tripleo-heat-templates
...
Overcloud Deployed
</pre>
<p>Here is our OSDs tree, with two instances running on the node with two rotatory disks (sharing the same journal disk):</p>
<pre class="literal-block">
oc$ ceph os tree
ID WEIGHT  TYPE NAME                        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.03119 root default
-2 0.00780     host overcloud-cephstorage-1
 0 0.00780         osd.0                         up  1.00000          1.00000
-3 0.01559     host overcloud-cephstorage-2
 1 0.00780         osd.1                         up  1.00000          1.00000
 2 0.00780         osd.2                         up  1.00000          1.00000
-4 0.00780     host overcloud-cephstorage-0
 3 0.00780         osd.3                         up  1.00000          1.00000
</pre>
<p>and the custom PG/size values for for &quot;vms&quot; pool:</p>
<pre class="literal-block">
oc$ ceph osd pool get vms size
size: 2
oc$ ceph osd pool get vms pg_num
pg_num: 128
</pre>
<p>Another simple customization could have been to set the journals size. For example:</p>
<pre class="literal-block">
ceph_journal_size.yaml
  parameter_defaults:
    ExtraConfig:
      ceph::profile::params::osd_journal_size: 1024
</pre>
<p>Also we did not provide any customization for the <cite>crushmap</cite> but <a class="reference external" href="https://github.com/openstack/puppet-ceph/commit/b1af406398488df7fc3d35263451f3e2ad802b9b">a recent addition</a> from <a class="reference external" href="https://twitter.com/epkuva">Erno</a> makes it possible to disable <code>global/osd_crush_update_on_start</code> so that any customization becomes possible after the deployment is finished.</p>
<p>Also we did not deploy the RadosGW service as it is still a work in progress, expected for the Newton release. Submissions for its inclusion are <a class="reference external" href="https://review.openstack.org/#/c/289027/">on review</a>.</p>
<p>We're also working on automating the upgrade from the Ceph/Hammer release deployed with TripleO/Mitaka to Ceph/Jewel, installed with TripleO/Newton. The process will be integrated with the OpenStack upgrade and again the submissions are <a class="reference external" href="https://review.openstack.org/#/c/359410/">on review in a series</a>.</p>
</div>
</div>
<div class="section" id="for-more-scenarios">
<h2>For more scenarios</h2>
<p>The mechanism recently introduced in TripleO to make composable roles, discussed in a <a class="reference external" href="http://hardysteven.blogspot.it/2016/08/tripleo-composable-services-101.html">Steven</a>'s blog post, makes it possible to test a complete Ceph deployment using a single controller node too (hosting the OSD service as well), just by adding <code>OS::TripleO::Services::CephOSD</code> to the list of services deployed on the controller role.</p>
<p>And if the above still wasn't enough, TripleO continues to support configuration of OpenStack with a pre-existing, unmanaged Ceph cluster. To do so we'll want to customize the parameters in <code>puppet-ceph-external.yaml</code> and deploy passing that as argument instead. For example:</p>
<pre class="literal-block">
puppet-ceph-external.yaml
  resource_registry:
    OS::TripleO::Services::CephExternal: tripleo-heat-templates/puppet/services/ceph-external.yaml

  parameter_defaults:
    # NOTE: These example parameters are required when using Ceph External and must be obtained from the running cluster
    #CephClusterFSID: '4b5c8c0a-ff60-454b-a1b4-9747aa737d19'
    #CephClientKey: 'AQDLOh1VgEp6FRAAFzT7Zw+Y9V6JJExQAsRnRQ=='
    #CephExternalMonHost: '172.16.1.7, 172.16.1.8'

    # the following parameters enable Ceph backends for Cinder, Glance, Gnocchi and Nova
    NovaEnableRbdBackend: true
    CinderEnableRbdBackend: true
    CinderBackupBackend: ceph
    GlanceBackend: rbd
    GnocchiBackend: rbd
    # If the Ceph pools which host VMs, Volumes and Images do not match these
    # names OR the client keyring to use is not named 'openstack',  edit the
    # following as needed.
    NovaRbdPoolName: vms
    CinderRbdPoolName: volumes
    GlanceRbdPoolName: images
    GnocchiRbdPoolName: metrics
    CephClientUserName: openstack
    # finally we disable the Cinder LVM backend
    CinderEnableIscsiBackend: false
</pre>
<p>Come help in #tripleo &#64; freenode and don't forget to check the docs at <a class="reference external" href="http://tripleo.org">tripleo.org</a>! Some related topics are described there, for example, how to <a class="reference external" href="http://tripleo.org/advanced_deployment/root_device.html">set the root device</a> via Ironic for the nodes with multiple disks or how to <a class="reference external" href="http://tripleo.org/advanced_deployment/ceph_config.html">push in ceph.conf</a> additional arbitraty settings.</p>
</div>

  </div>
  <div class="article_meta">
    <p>Category: <a href="http://gfidente.com/category/techie.html">techie</a>
 &ndash; Tags:
      <a href="http://gfidente.com/tag/openstack.html">openstack</a>,      <a href="http://gfidente.com/tag/ceph.html">ceph</a>,      <a href="http://gfidente.com/tag/tripleo.html">tripleo</a>    </p>
  </div>


</article>


    <div id="ending_message">
      <p>&copy; Giulio Fidente. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. Member of the <a href="http://internetdefenseleague.org">Internet Defense League</a>.</p>
    </div>
  </main>
<script type="text/javascript">
  window._idl = {};
  _idl.variant = "banner";
  (function() {
    var idl = document.createElement('script');
    idl.type = 'text/javascript';
    idl.async = true;
    idl.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'members.internetdefenseleague.org/include/?url=' + (_idl.url || '') + '&campaign=' + (_idl.campaign || '') + '&variant=' + (_idl.variant || 'banner');
    document.getElementsByTagName('body')[0].appendChild(idl);
  })();
</script>
</body>
</html>