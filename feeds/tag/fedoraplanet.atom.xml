<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Giulio Fidente - fedoraplanet</title><link href="http://giuliofidente.com/" rel="alternate"></link><link href="http://giuliofidente.com/feeds/tag/fedoraplanet.atom.xml" rel="self"></link><id>http://giuliofidente.com/</id><updated>2015-11-03T17:30:00+01:00</updated><entry><title>OpenStack summit in Tokyo and TripleO</title><link href="http://giuliofidente.com/2015/11/openstack-summit-in-tokyo-and-tripleo.html" rel="alternate"></link><published>2015-11-03T17:30:00+01:00</published><updated>2015-11-03T17:30:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-11-03:/2015/11/openstack-summit-in-tokyo-and-tripleo.html</id><summary type="html">&lt;p&gt;&lt;a class="reference external" href="https://www.openstack.org/summit/tokyo-2015/"&gt;OpenStack summit Tokyo&lt;/a&gt; anyone? I've been there and thought it was a very well organized event, in a nice location. Every minute together with peers seemed worth it to me. This said, let's talk about the actual sessions. I spent most of my time at the TripleO and Heat sessions, with a little detour on Magnum. Plus some booth crawling.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;TripleO&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;We had three sessions and a full day meetup. The first discussed the recent â€¦&lt;/p&gt;&lt;/dd&gt;&lt;/dl&gt;</summary><content type="html">&lt;p&gt;&lt;a class="reference external" href="https://www.openstack.org/summit/tokyo-2015/"&gt;OpenStack summit Tokyo&lt;/a&gt; anyone? I've been there and thought it was a very well organized event, in a nice location. Every minute together with peers seemed worth it to me. This said, let's talk about the actual sessions. I spent most of my time at the TripleO and Heat sessions, with a little detour on Magnum. Plus some booth crawling.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;TripleO&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;We had three sessions and a full day meetup. The first discussed the recent work done to allow for the deployment of OpenStack in containers, using TripleO; the changes to deploy the compute nodes and a majority of the controller roles in containers are up for review, which is great; some notes can be found in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-containers"&gt;tripleo-mitaka-containers&lt;/a&gt; pad. It was very nice for me to see the TripleO bits delivering on the promise of being flexibile; deploying in containers requires minimal changes to the core tools and yet inherits important features like network isolation and support for Ceph.&lt;/p&gt;
&lt;p&gt;Another session was on upgrades, which isn't a trivial goal as it pushes on both Heat and Puppet. The proposed plan is to start with a CI job attempting to upgrade an overcloud from the stable branch to the master branch; more in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-kilo-to-liberty-upgrades"&gt;tripleo-kilo-to-liberty-upgrades&lt;/a&gt; pad. I will hopefully write more about this topic in future posts.&lt;/p&gt;
&lt;p&gt;There has also been a session on the CLI and UI clients. Again, not a simple problem because the Heat templates are constantly ongoing refactor to implement new features and because putting too much business logic into the client limited our flexibility in the past. We'll probably see a shared library for the newer CLI/UI, with less business logic and probably a REST API, more in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-restapi"&gt;tripleo-mitaka-restapi&lt;/a&gt; pad.&lt;/p&gt;
&lt;p class="last"&gt;Then the community meetup on Friday covered a few additional topics. We reviewed the status of the CI with ideas for more jobs, discussed some ideas to provision the puppet modules via Heat instead of shipping them with the images, collected lots of good feedback from actual OpenStack operators and, last but not least, discussed the recent work from &lt;a class="reference external" href="https://github.com/dprince/"&gt;Dan Prince&lt;/a&gt; to have composable roles. Kudos to Dan on the composable roles which also seems to work pretty well with the effort to containerize the deployment. Notes about the community meetup are in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-meetup"&gt;tripleo-mitaka-meetup&lt;/a&gt; pad.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Heat&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;I've joined only those sessions were I could at least understand the topics and yet there is quite a lot to write about. First an interesting session focused on a problem with dependencies across nested stacks, which we make large us of in TripleO. Both the issue and the consequences are much easier to understand with a picture than with words. &lt;a class="reference external" href="http://www.zerobanana.com/"&gt;Zane Bitter&lt;/a&gt; had indeed prepared some &lt;em&gt;demo&lt;/em&gt; material for the session, including pictures, which should be linked from the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-break-stack-barrier"&gt;mitaka-heat-break-stack-barrier&lt;/a&gt; pad.&lt;/p&gt;
&lt;p&gt;Then a session to discuss issues affecting large stack deployments. Major points for discussion were of three types: token timeouts, batching of operations and resources status polling, with resources polling being probably the hardest to cope with. More in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-large-stacks"&gt;mitaka-heat-large-stacks&lt;/a&gt; pad.&lt;/p&gt;
&lt;p&gt;Then a session to collect input from users and ops. My proposal here was for the introduction of 'immutable resources' where resources with such a proerty wouldn't get deleted or updated during a stack update. One proposed solution for this is a warning message, outputted by the stack validation process, in case of resources deletion. More on this and on the session in general in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-user-ops"&gt;mitaka-heat-user-ops&lt;/a&gt; pad.&lt;/p&gt;
&lt;p class="last"&gt;There has also been an interesting conversation about the changes from a spec meant to introduce support for &lt;a class="reference external" href="http://specs.openstack.org/openstack/heat-specs/specs/liberty/external_resource.html"&gt;external resources&lt;/a&gt;, which is something TripleO could use soon.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Magnum&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;The Magnum project seems to be facing some of the problems which TripleO faced in past as well, in relation to the Heat templates. The goal seems to be to customize the templates based on the input parameters; I joined the session to learn more about the Magnum approach and the proposed solutions.&lt;/p&gt;
&lt;p class="last"&gt;It seems to me that while TripleO relies on actual Heat features to allow for different implementations of the same resource, via resource registry, Magnum is working instead on a sort of pre-processor (powered by Jinja) which generates Heat templates using conditionals and includes from the Jinja language. More on this is in the &lt;a class="reference external" href="https://bugs.launchpad.net/magnum/+bug/1501045"&gt;launchpad bug 1501045&lt;/a&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Who said installers?&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;It was nice to see TripleO mentioned in at least two more sessions, one where Mirantis directly compared Fuel to the Red Hat OpenStack Platform Director and another where &lt;a class="reference external" href="https://openstacksummitoctober2015tokyo.sched.org/event/7032267fad9f6f0e5242093d17d59d64"&gt;four different automated installers were reviewed&lt;/a&gt;. Each had its moments.&lt;/p&gt;
&lt;p class="last"&gt;I think one good thing about TripleO is that it is pretty flexible and offers some capabilities to maintain the environment even after the initial installation; hopefully both will make it interesting to many.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;RDO meetup&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;We had a meetup for the &lt;a class="reference external" href="http://rdoproject.org/"&gt;RDO&lt;/a&gt; project and it cleared up a lot of questions on how the packages are built and tested for both the trunk and the stable branches. Good news is that it is now possible to try TripleO, on CentOS, with the latest RDO bits. The docs at &lt;a class="reference external" href="http://tripleo.org"&gt;tripleo.org&lt;/a&gt; are documenting exactly that.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now, what about the event? Well, as an ATC, I have two things to say about the event more in general. First, &lt;strong&gt;I'd really like the design summit to start on Tuesday&lt;/strong&gt;. Simply put, there are so many sessions going on that it is impossible to join anything out of the most immediate interests. My thinking is that one more day could at least lower parallelism!&lt;/p&gt;
&lt;p&gt;Another comment is, please &lt;strong&gt;make the keynotes a little less marketish&lt;/strong&gt;. I know the circus runs on actual money but my feeling is that the keynotes, on both Tuesday and Wednesday, left some sessions go a little too far into the product marketing area when they were probably supposed to present an actual OpenStack use case.&lt;/p&gt;
&lt;img alt="OpenStack summit Ticket" class="align-center" src="http://giuliofidente.com/images/ticket_os2015.png" /&gt;
&lt;p&gt;Last but not least, it was great to have some time to meet face to face peers and coworkers from all over the world. Including some who are &lt;em&gt;not exactly&lt;/em&gt; peers, like &lt;a class="reference external" href="https://www.linkedin.com/in/matthicksj"&gt;Matt&lt;/a&gt;, to whom I ended up asking if he was a newcome to the TripleO project during dinner. I'm sure next time I will remember and do better.&lt;/p&gt;
</content><category term="openstack"></category><category term="events"></category><category term="tripleo"></category><category term="fedoraplanet"></category></entry><entry><title>On TripleO and the EnablePacemaker bool</title><link href="http://giuliofidente.com/2015/05/on-tripleo-and-the-enablepacemaker-bool.html" rel="alternate"></link><published>2015-05-01T15:02:00+02:00</published><updated>2015-05-01T15:02:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-05-01:/2015/05/on-tripleo-and-the-enablepacemaker-bool.html</id><summary type="html">&lt;p&gt;Yes, EnablePacemaker! A little old fashioned and not very well suited for OpenStack you say heh? Let's try to talk a bit about this then. News is that we're adding into TripleO/Puppet an option to deploy Pacemaker on the Overcloud nodes. Puppet you said? Sure TripleO can use Puppet now for the configuration of the Overcloud nodes, ask Dan Prince :) Actually usage of Puppet is nowdays the preferred approach and many features are available â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yes, EnablePacemaker! A little old fashioned and not very well suited for OpenStack you say heh? Let's try to talk a bit about this then. News is that we're adding into TripleO/Puppet an option to deploy Pacemaker on the Overcloud nodes. Puppet you said? Sure TripleO can use Puppet now for the configuration of the Overcloud nodes, ask Dan Prince :) Actually usage of Puppet is nowdays the preferred approach and many features are available only in the Puppet scenario ... amongst which the EnablePacemaker bool in subject.&lt;/p&gt;
&lt;p&gt;To go back to Pacemaker, no this won't be the Active/Passive implementation described in the OpenStack HA docs. Not at all, there are no special resource agents for the OpenStack services and more importantly, there is no Active/Passive, when not needed. Instead, the architecture looks a lot more like the Active/Active scenario described in tha doc, except there is Pacemaker coping with a few uncovered bits.&lt;/p&gt;
&lt;p&gt;Whoever tried to setup a RabbitMQ cluster from scratch&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;dan prince&lt;/li&gt;
&lt;li&gt;ha docs&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;####### OLD STUFF #######&lt;/p&gt;
&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite the OpenStack services being designed around a scale-out concept, while attempting to achieve that in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we spotted a number of weak angles, some of which could be worked around, others instead still need some changes in the core service. You're encouraged to try what we have available today and help with the rest.&lt;/p&gt;
&lt;p&gt;So to try out OpenStack HA with &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; you just set a number &amp;gt;= 3 for &lt;code&gt;OVERCLOUD_CONTROLSCALE&lt;/code&gt; and continue with &lt;a class="reference external" href="http://docs.openstack.org/developer/tripleo-incubator/devtest.html"&gt;devtest&lt;/a&gt; as usual. Nodes will be configured appropriately:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export OVERCLOUD_CONTROLSCALE=3
source scripts/devtest_variables.sh
...
&lt;/pre&gt;
&lt;p&gt;Don't forget this is only tested on a few distros for now, I'd pick some Fedora 20.&lt;/p&gt;
&lt;p&gt;On the controller nodes, MariaDB with Galera (for Fedora) is going to provide for a reliable SQL. There is still some work in progress to make sure the Galera cluster can be restarted correctly should all the controllers go down at the same time but, for single node failures, this should be safe to use.&lt;/p&gt;
&lt;p&gt;RabbitMQ nodes are clustered and balanced (via HAProxy), queues replicated.&lt;/p&gt;
&lt;p&gt;And with regards to the OpenStack services, these are configured in a balancing manner (again, using HAProxy) except for those cases where this wouldn't have worked, notably the Neutron L3 agent and the Ceilometer Central agent, yet these are under control via Pacemaker and a single instance is expected to be running at all times. Cinder instead remains uncovered as volumes would require a shared storage for proper HA. A spec has been proposed for this though.&lt;/p&gt;
&lt;p&gt;Also, behind the scenes, the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; template language addon shipped as &lt;em&gt;merge.py&lt;/em&gt; and included in &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates"&gt;tripleo-heat-templates&lt;/a&gt;, which allows for example for scaling of the resources definition, is currently going to be removed and replaced with code living entirely in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And there is more so once you tried, join us on #tripleo &amp;#64; freenode for the real fun!&lt;/p&gt;
</content><category term="openstack"></category><category term="tripleo"></category><category term="high availability"></category><category term="pacemaker"></category><category term="fedoraplanet"></category></entry><entry><title>Ceph for Cinder in TripleO</title><link href="http://giuliofidente.com/2015/01/ceph-for-cinder-in-tripleo.html" rel="alternate"></link><published>2015-01-06T17:15:00+01:00</published><updated>2015-01-06T17:15:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-01-06:/2015/01/ceph-for-cinder-in-tripleo.html</id><summary type="html">&lt;p&gt;A wrap up on the status of TripleO's Cinder HA spec. First, a link to the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, where you can find even more links, to the actual spec (under review) and the code changes (again, still under review). Intent of the blueprint is for the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; deployments to keep Cinder volumes available and Cinder operational in case of failures of any node.&lt;/p&gt;
&lt;p&gt;This said, should $subject sound interesting to you, beware the code still â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;A wrap up on the status of TripleO's Cinder HA spec. First, a link to the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, where you can find even more links, to the actual spec (under review) and the code changes (again, still under review). Intent of the blueprint is for the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; deployments to keep Cinder volumes available and Cinder operational in case of failures of any node.&lt;/p&gt;
&lt;p&gt;This said, should $subject sound interesting to you, beware the code still needs reviews, probably polishing and surely more features ... so take this post as a call for help as well! Now some details.&lt;/p&gt;
&lt;p&gt;For Cinder to remain operational, in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we deploy the &lt;code&gt;cinder-{api,schduler,volume}&lt;/code&gt; services on all controller nodes (as it was even before the proposed spec) but we also customize the &lt;code&gt;host&lt;/code&gt; setting so that all instances of &lt;code&gt;cinder-volume&lt;/code&gt; will share an identical identifier. This is so that all instances can perform operations on all volumes, otherwise, the particular host which created a volume would be the only one capable of performing further operations on it. This exposed some potential issues with Cinder due to concurrent mutation of same resource from multiple nodes and they already are taken care of, in Cinder, with the &lt;a class="reference external" href="https://blueprints.launchpad.net/cinder/+spec/cinder-state-enforcer"&gt;state-enforcer blueprint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then, for the volumes to remain available, &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; will deploy a Ceph cluster together with the OpenStack components, to be used as a backend for Cinder. Each and every controller will host a Ceph monitor while an arbitrary number of additional nodes can be configured as Ceph OSDs. The basic idea is that by doing so people will be able to scale out the number of controllers and/or data nodes independently, in an attempt to suit the needs for more space or more CPU resources depending on the use case. In addition to that, network partitioning should also be easy to achieve as traffic from Nova nodes will be directed to the OSDs nodes only for volumes I/O, not to the controllers. Note though that the existing implementation &lt;strong&gt;does not yet&lt;/strong&gt; implement support for the network partitioning but, on a good note, &lt;strong&gt;it does&lt;/strong&gt; use cephx already to secure access to data.&lt;/p&gt;
&lt;p&gt;As of today, assuming you checkout by yourself, as linked from the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, the needed &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; changes, everything should &lt;em&gt;just work&lt;/em&gt; by setting some value &amp;gt; 0 for the &lt;code&gt;CEPHSTORAGESCALE&lt;/code&gt; env variable, which represents the number of OSD nodes you want to deploy. Should you need it, in the Heat templates it is possible to customize a number of useful Ceph settings, like the number of replicas for the data (needs to be lower than the number of OSD nodes). Now go try it out and come back with some feedback! :)&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="tripleo"></category><category term="ceph"></category><category term="high availability"></category><category term="fedoraplanet"></category></entry><entry><title>TripleO vs OpenStack HA</title><link href="http://giuliofidente.com/2014/08/tripleo-vs-openstack-ha.html" rel="alternate"></link><published>2014-08-21T15:02:00+02:00</published><updated>2014-08-21T15:02:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2014-08-21:/2014/08/tripleo-vs-openstack-ha.html</id><summary type="html">&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite the OpenStack services being designed around a scale-out concept, while attempting to achieve that in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we spotted a number of weak angles, some of which could be worked around, others instead still need some changes in the core service. You're encouraged to try what we have available today and help with the rest.&lt;/p&gt;
&lt;p&gt;So to try out OpenStack HA with &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; you just set a number &amp;gt;= 3 for &lt;code&gt;OVERCLOUD_CONTROLSCALE&lt;/code&gt; and continue with &lt;a class="reference external" href="http://docs.openstack.org/developer/tripleo-incubator/devtest.html"&gt;devtest&lt;/a&gt; as usual. Nodes will be configured appropriately:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export OVERCLOUD_CONTROLSCALE=3
source scripts/devtest_variables.sh
...
&lt;/pre&gt;
&lt;p&gt;Don't forget this is only tested on a few distros for now, I'd pick some Fedora 20.&lt;/p&gt;
&lt;p&gt;On the controller nodes, MariaDB with Galera (for Fedora) is going to provide for a reliable SQL. There is still some work in progress to make sure the Galera cluster can be restarted correctly should all the controllers go down at the same time but, for single node failures, this should be safe to use.&lt;/p&gt;
&lt;p&gt;RabbitMQ nodes are clustered and balanced (via HAProxy), queues replicated.&lt;/p&gt;
&lt;p&gt;And with regards to the OpenStack services, these are configured in a balancing manner (again, using HAProxy) except for those cases where this wouldn't have worked, notably the Neutron L3 agent and the Ceilometer Central agent, yet these are under control via Pacemaker and a single instance is expected to be running at all times. Cinder instead remains uncovered as volumes would require a shared storage for proper HA. A spec has been proposed for this though.&lt;/p&gt;
&lt;p&gt;Also, behind the scenes, the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; template language addon shipped as &lt;em&gt;merge.py&lt;/em&gt; and included in &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates"&gt;tripleo-heat-templates&lt;/a&gt;, which allows for example for scaling of the resources definition, is currently going to be removed and replaced with code living entirely in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And there is more so once you tried, join us on #tripleo &amp;#64; freenode for the real fun!&lt;/p&gt;
</content><category term="openstack"></category><category term="tripleo"></category><category term="high availability"></category><category term="meetup"></category><category term="fedoraplanet"></category></entry><entry><title>My takeaways from EuroPython 2013</title><link href="http://giuliofidente.com/2013/07/my-takeaways-from-europython-2013.html" rel="alternate"></link><published>2013-07-06T17:10:00+02:00</published><updated>2013-07-06T17:10:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-07-06:/2013/07/my-takeaways-from-europython-2013.html</id><summary type="html">&lt;p&gt;Been there! The event was very well organized and even the food was good. The following are the talks I attended with some comments and my takeaways.&lt;/p&gt;
&lt;p&gt;Despite the &lt;a class="reference external" href="http://europython.eu"&gt;EuroPython&lt;/a&gt; 2013 lasting a full week, including the weekend with some code sprints, I could only join the event for three days, from Tuesday to Thursday. Still it was a great experience and a good chance to learn about new things while also meet people who â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Been there! The event was very well organized and even the food was good. The following are the talks I attended with some comments and my takeaways.&lt;/p&gt;
&lt;p&gt;Despite the &lt;a class="reference external" href="http://europython.eu"&gt;EuroPython&lt;/a&gt; 2013 lasting a full week, including the weekend with some code sprints, I could only join the event for three days, from Tuesday to Thursday. Still it was a great experience and a good chance to learn about new things while also meet people who I only knew because of IRC, by their nicknames. The &lt;a class="reference external" href="https://ep2013.europython.eu/p3/schedule/ep2013/"&gt;schedule&lt;/a&gt; was quite tight though and there were talks in all five rooms at the same time, making it difficult to join all the talks I wanted. This is the best I could came up with.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;I see OpenStack in your future!&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Being a stacker, I couldn't miss this. &lt;a class="reference external" href="http://muharem.wordpress.com/"&gt;Muharem&lt;/a&gt; gave some overview of the components for newcomers but mainly the talk was a discussion around the project surroundings, the community, the developments. I think it was very good even for knowledgeable. Almost 300 single contributors for Grzzily with Red Hat currently at the top of the list and IBM saying the community is growing by the trends seen for Linux, only at ten times the speed!&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Introduction to OpenStack Swift&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Again, I couldn't miss this. &lt;a class="reference external" href="http://blog.chmouel.com/"&gt;Chmouel&lt;/a&gt; works on Swift from some time now and during the speech gave some insights on its internals, with the &lt;cite&gt;ring&lt;/cite&gt; being the most interesting thing to me. It basically keeps track of where all data resides pointing to the particular device on the particular physical node. After the talk I even had a chance to talk more to him and go trough some ideas/guidelines to scale all the core OpenStack components, not just Swift. This was enlightening and pointed us to a few must have: regions, database replicas, good network connectivity.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Marconi: Queuing and Notification service for OpenStack&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;An interesting talk about the incubated component called Marconi, which aims at introducing support for Queuing and Notifications in OpenStack, as per Amazon's SQS and SNS services. &lt;a class="reference external" href="http://blog.flaper87.org/"&gt;Flavio&lt;/a&gt; did a great job in showing why, while still in its early days, Marconi with the current MongoDB storage driver implementation, is going to bring into the core group a component well designed for scalability from day one. The team is small and looking for contributors but the project adds an important piece to the OpenStack puzzle; I look forward to put my hands on it.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Introduction to machine learning using Python tools&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;The room was full. Not only that, some were standing on their feets for all the 50mins. I had some previous exposure to the machine learning concepts thanks to the &lt;a class="reference external" href="https://www.coursera.org/course/ml"&gt;Machine Learning Class&lt;/a&gt; from Coursera but &lt;a class="reference external" href="http://shankar.bigbig.com/"&gt;Satish&lt;/a&gt; went trough a recap of all those and also introduced a few python tools showing some of their capabilities by using real world data! There are very good tools around with sickit-learn being probably one of the most complete but, for their correct usage, one needs to &lt;cite&gt;know&lt;/cite&gt; the data. I see me digging a bit further into the clustering algorithms as soon as a good occasion pops up. This was a very well structured talk with many ways out for all the people attending I suppose.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Will iPython replace bash?&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;I wonder why it took me so much time to hear about iPython! &lt;a class="reference external" href="http://vaunaspada.babel.it/"&gt;Roberto&lt;/a&gt; (a former colleague of mine) jokingly invited people to use iPython as an actual shell in place of the regular bash. Actually, in the cloud era, while we move from pets to cattle and have to deal with so many systems at once, having a more manageable, portable and reliable way to write &lt;cite&gt;scripts&lt;/cite&gt; is important and python fits very well into the purpose. iPython seems a perfect complimentary.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Greenlet-based concurrency&lt;/strong&gt; and &lt;strong&gt;Taming greenlets using eventlet&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;I had to face threads in python when trying to parallelize the execution of some long running tests. The experience was okay but later I told myself I should have tried greenlet. Both these talks were the perfect occasion to learn more about it. Also, the eventlet tutorial / code snippet online makes it very appealing but the talk gave a lot of insights on how the underlying code works. This monkey_patch thing for instance seems the way to go for pre-existing code but people was descouraged from using it for anything which can be quickly refactored. The thing is, greenlet make you run lighter; eventlet make the code easy to maintain and more efficient thanks to the &lt;cite&gt;switch when needed&lt;/cite&gt; concept. The two combined are for the good of the most common parallelization use cases. Kudos to &lt;a class="reference external" href="http://www.goranperetin.com/"&gt;Goran&lt;/a&gt; and &lt;a class="reference external" href="http://devork.be/"&gt;Floris&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;OpenStack on OpenStack: Deploying OpenStack using OpenStack&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;This TripleO thing was known to me but never had the chance to try it out. The fact that it uses Heat makes it easily extensible and definitely an interesting educational project for me but I still don't see many real world cases for it. Given that both the underlying (bare) and upper layer configuration may vary a lot, I expect a number bits and pieces to require some manual set up anyway. In addition to that a state management system, like Salt, is also needed for post deployment management. Still this pushes around the edges of what is possible to automate today.&lt;/dd&gt;
&lt;/dl&gt;
&lt;img alt="EuroPython Ticket" class="align-right" src="http://giuliofidente.com/images/ticket_ep2013.png" /&gt;
&lt;p&gt;I managed to attend a 1 hour long session of lightning talks too, a nice experience. I wanted to provide some links to an awesome project discussed in that occasion, a mobile app aimed at helping mute people communicate, providing icons and language processing facilities. Unfortunately I don't seem to find any reference to it online! :(&lt;/p&gt;
&lt;p&gt;One more thing, amongst the gadgets given to the participants there was a very good one, a book: &lt;a class="reference external" href="http://python3porting.com/"&gt;Porting to Python 3&lt;/a&gt;. You can read it for free online but having it printed is very convenient. I found the book a good pick and a very useful read, given the in-depth details which are discussed in only a few pages.&lt;/p&gt;
&lt;p&gt;Last but not least, next year the &lt;a class="reference external" href="http://europython.eu"&gt;EuroPython&lt;/a&gt; will be held in Berlin! The only unfortunate thing for this year event were some problems with the &lt;a class="reference external" href="https://ep2013.europython.eu/blog/2013/05/20/enjoy-your-evenings"&gt;PyBarbecue&lt;/a&gt;, due probably to too many people joining.&lt;/p&gt;
</content><category term="europython"></category><category term="python"></category><category term="events"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Glance - Use Swift as backend</title><link href="http://giuliofidente.com/2013/06/openstack-glance-use-swift-as-backend.html" rel="alternate"></link><published>2013-06-27T02:55:00+02:00</published><updated>2013-06-27T02:55:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-27:/2013/06/openstack-glance-use-swift-as-backend.html</id><summary type="html">&lt;p&gt;On &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; again. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; is the component in charge of hosting the images (and image snapshots) to be cloned for the ephemeral instances. Images usually are just some random big files so it makes perfect sense to use &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt; for such an object (a File Object storage)!&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; image node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This said â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;On &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; again. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; is the component in charge of hosting the images (and image snapshots) to be cloned for the ephemeral instances. Images usually are just some random big files so it makes perfect sense to use &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt; for such an object (a File Object storage)!&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; image node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This said, only few changes are needed to swap from local filesystem storage to &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;. Edit the &lt;code&gt;glance-api.conf&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
default_store = swift
swift_store_auth_address = $KEYSTONE_PROTOCOL://$KEYSTONE_HOST:$KEYSTONE_PORT/v2.0/
swift_store_user = $SERVICE_TENANT_NAME:glance
swift_store_key = $SERVICE_PASSWORD
swift_store_create_container_on_put = True
&lt;/pre&gt;
&lt;p&gt;These are probably self-explanatory but I have a few tips to spare! If you decide to go via https for the keystone service, make sure you can validate locally (on &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt;) the https certificate. If unsure about the values to be used for the $SERVICE_* variables, these are the same set in the same config file in section &lt;code&gt;keystone_authtoken&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (Dec 2013):&lt;/strong&gt; The user you will set as &lt;code&gt;swift_store_user&lt;/code&gt; must have rights to create new containers in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;, to have that you can assign it the &lt;code&gt;ResellerAdmin&lt;/code&gt; role.&lt;/p&gt;
&lt;p&gt;Also, while not needed, you should consider using port 35357 rather than 5000 for the KEYSTONE_PORT as it is the port where administrative commands can be given.&lt;/p&gt;
&lt;p&gt;Short and straight to the point!&lt;/p&gt;
</content><category term="openstack"></category><category term="glance"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Configure multiple backends</title><link href="http://giuliofidente.com/2013/06/openstack-cinder-configure-multiple-backends.html" rel="alternate"></link><published>2013-06-16T16:37:00+02:00</published><updated>2013-06-16T16:37:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-16:/2013/06/openstack-cinder-configure-multiple-backends.html</id><summary type="html">&lt;p&gt;Following &lt;a class="reference external" href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html"&gt;my first post of the series&lt;/a&gt; discussing how to scale &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; to multiple nodes, with this I want to approach the configuration and usage of the multibackend feature landed in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; with the Grizzly release.&lt;/p&gt;
&lt;p&gt;This feature allows you to configure a single volume node for use with more than a single backend driver. You can find all about the few configuration bits needed also in the &lt;a class="reference external" href="http://docs.openstack.org/trunk/openstack-block-storage/admin/content/multi_backend.html"&gt;OpenStack block storage documentation&lt;/a&gt;. That makes â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Following &lt;a class="reference external" href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html"&gt;my first post of the series&lt;/a&gt; discussing how to scale &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; to multiple nodes, with this I want to approach the configuration and usage of the multibackend feature landed in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; with the Grizzly release.&lt;/p&gt;
&lt;p&gt;This feature allows you to configure a single volume node for use with more than a single backend driver. You can find all about the few configuration bits needed also in the &lt;a class="reference external" href="http://docs.openstack.org/trunk/openstack-block-storage/admin/content/multi_backend.html"&gt;OpenStack block storage documentation&lt;/a&gt;. That makes this post somehow redundant but I wanted to keep up with the series and the topic is well worth to be kept also here.&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; volume node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assuming we want our node, configured with some LVM based and an additional NFS based backend, this is what we would need to add into &lt;code&gt;cinder.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
enabled_backends=lvm1,nfs1
[lvm1]
volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver
volume_backend_name=LVM_iSCSI
[nfs1]
nfs_shares_config=${PATH_TO_YOUR_SHARES_FILE}
volume_driver=cinder.volume.drivers.nfs.NfsDriver
volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;enabled_backends&lt;/code&gt; value defines some names (separated by a comma) for the config groups. These do not have to match the driver name nor the backend name.&lt;/p&gt;
&lt;p&gt;When the configuration is complete, to use a particular backend when allocating new volumes, you'll have to pass a &lt;code&gt;volume_type&lt;/code&gt; parameter to the creation command. Such a type has to be created beforehand and to have some backends assigned to it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder type-create lvm
# cinder type-key lvm set volume_backend_name=LVM_iSCSI
# cinder type-create nfs
# cinder type-key nfs set volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;Finally, to create your volumes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder create --volume_type lvm --display_name inlvm 1
&lt;/pre&gt;
&lt;p&gt;For people using the REST interface, to set any &lt;code&gt;type-key&lt;/code&gt; property, including &lt;code&gt;volume_backend_name&lt;/code&gt;, you pass that information along with the request as &lt;a class="reference external" href="https://github.com/openstack/cinder/blob/master/cinder/api/contrib/types_extra_specs.py"&gt;extra specs&lt;/a&gt;. You can list those indeed to make sure the configuration is working as expected:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
#  cinder extra-specs-list
&lt;/pre&gt;
&lt;p&gt;Note that you can have backends of the same type (driver) using different names (say two LVM based backends allocating volumes in different volume groups) or you can also have backends of the same type using the same name! The scheduler is in charge of making the proper decision on how to pickup the correct backend at creation time so a few notes on the filter scheduler (enabled by default in Grizzly):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;firstly it filters the available backends (AvailabilityZoneFilter, CapacityFilter and CapabilitiesFilter are enabled by default and the backend name is matched against the capabilities)&lt;/li&gt;
&lt;li&gt;secondly weights the previously filtered backends (CapacityWeigher is the only one enabled by default)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The CapacityWeigher attributes high score to backends with the most available space, so new volumes are allocated within the backend with the more space available matching the particular name in the request.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE (Nov 2013):&lt;/strong&gt; As reported by Yogev &lt;a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1031010"&gt;in this bug&lt;/a&gt;, misplacing the settings can have dangerous side effects. All settings below the &lt;code&gt;enabled_backends&lt;/code&gt; parameter are actually in some section (eg. [lvm1]) of the ini file rather than [DEFAULT]. Make sure to move the [lvm1] and [nfs1] settings to the bottom of the file and so that all other settings are in the [DEFAULT] section.&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Add more volume nodes</title><link href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html" rel="alternate"></link><published>2013-04-30T02:00:00+02:00</published><updated>2013-04-30T02:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-30:/2013/04/openstack-cinder-add-more-volume-nodes.html</id><summary type="html">&lt;p&gt;With this being the first of a short series, I'd like to publish some articles intendend to cover the required steps to configure &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; (&lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; block storage service) in a mid/large deployment scenario. The idea is to discuss at least three topics: how to scale the service by adding more volume nodes; how to ensure high-availablity for the API and Scheduler sub-services; leverage the multi-backend feature landed in Grizzly.&lt;/p&gt;
&lt;p&gt;I'm starting with this post â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;With this being the first of a short series, I'd like to publish some articles intendend to cover the required steps to configure &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; (&lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; block storage service) in a mid/large deployment scenario. The idea is to discuss at least three topics: how to scale the service by adding more volume nodes; how to ensure high-availablity for the API and Scheduler sub-services; leverage the multi-backend feature landed in Grizzly.&lt;/p&gt;
&lt;p&gt;I'm starting with this post on the scaling issue first. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; is composed of three main parts, the API server, the scheduler and the volume service. The volume service is some sort of abstraction layer between the API and the actual resources provider.&lt;/p&gt;
&lt;p&gt;By adding more volume nodes into the environment you will be able to increase the total offering of block storage to the tenants. Each volume node can either provide volumes by allocating them locally or on a remote container like an NFS or GlusterFS share.&lt;/p&gt;
&lt;p&gt;Some assumptions before getting into the practice:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general OpenStack architecture&lt;/li&gt;
&lt;li&gt;you have at least one Cinder node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to do on the candidate node is to install the required packages. I'm running the examples on CentOS and using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository which makes this step as simple as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-cinder
&lt;/pre&gt;
&lt;p&gt;If you plan to host new volumes using the locally available storage dont' forget to create a volume group called &lt;code&gt;cinder-volumes&lt;/code&gt; (the name can be configured via the &lt;code&gt;cinder_volume&lt;/code&gt; parameter). Also don't forget to configure the &lt;code&gt;tgtd&lt;/code&gt; to include the config files created dynamically by &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt;. Add a line like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
include /etc/cinder/volumes/*
&lt;/pre&gt;
&lt;p&gt;in your &lt;code&gt;/etc/tgt/targets.conf&lt;/code&gt; file. Now enable and start the &lt;code&gt;tgtd&lt;/code&gt; service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# chkconfig tgtd on
# service tgtd start
&lt;/pre&gt;
&lt;p&gt;Amongst the three init services installed by &lt;code&gt;openstack-cinder&lt;/code&gt; you only need to run &lt;code&gt;openstack-cinder-volume&lt;/code&gt;, which gets configured in &lt;code&gt;/etc/cinder/cinder.conf&lt;/code&gt;. Configure it to connect to the existing &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; database (the db in use by the pre-existing node) and to the existing AMQP broker (again, in use by the pre-existing node) by setting the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection=mysql://cinder:${CINDER_DB_PASSWORD}&amp;#64;${CINDER_DB_HOST}/cinder
qpid_hostname=${QPIDD_BROKER}
&lt;/pre&gt;
&lt;p&gt;Set the credentials if needed and/or change the &lt;code&gt;rpc_backend&lt;/code&gt; setting if you're not using &lt;a class="reference external" href="http://qpid.apache.org/"&gt;Qpid&lt;/a&gt; as your message broker. One more setting, not really required to change but worth checking if you're using the local resources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
iscsi_ip_address=${TGTD_IP_ADDRESS}
&lt;/pre&gt;
&lt;p&gt;That should match the public ip address of the volume node just installed. The iSCSI targets created locally using &lt;code&gt;tgtadm/tgtd&lt;/code&gt; have to be reachable by the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Nova"&gt;Nova&lt;/a&gt; nodes. The IP address of each target is stored in the database with every volume created. The &lt;code&gt;iscsi_ip_address&lt;/code&gt; prameter sets what is the IP address to be given to the initiators.&lt;/p&gt;
&lt;p&gt;At this point you should be ready to start the volume service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# service openstack-cinder-volume start
&lt;/pre&gt;
&lt;p&gt;Verify that it started by checking the logs (&lt;code&gt;/var/log/cinder/volume.log&lt;/code&gt;) or by issueing on any &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; node:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder-manage host list
&lt;/pre&gt;
&lt;p&gt;you should see all of your volume nodes listed. From now on you can create new volumes as usual and they will be allocated on any of the volume nodes, keep in mind that the scheduler will default to the node with the most space available.&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>Getting to know and use Emacs better</title><link href="http://giuliofidente.com/2013/04/getting-to-know-and-use-emacs-better.html" rel="alternate"></link><published>2013-04-16T18:00:00+02:00</published><updated>2013-04-16T18:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-16:/2013/04/getting-to-know-and-use-emacs-better.html</id><summary type="html">&lt;p&gt;I know there are plenty of &lt;a class="reference external" href="http://emacsblog.org"&gt;Emacs related blogs&lt;/a&gt; discussing every single trick (&lt;a class="reference external" href="http://emacsredux.com/"&gt;including the easter eggs&lt;/a&gt;) so this won't be another. I don't have the skills for that either but I got to know Emacs better recently and decided to share my (hopefully nicely) commented &lt;code&gt;init.el&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Why? Because it took me some time to find in the Emacs docs what I wanted. A readable, well commented config file would have helped so â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I know there are plenty of &lt;a class="reference external" href="http://emacsblog.org"&gt;Emacs related blogs&lt;/a&gt; discussing every single trick (&lt;a class="reference external" href="http://emacsredux.com/"&gt;including the easter eggs&lt;/a&gt;) so this won't be another. I don't have the skills for that either but I got to know Emacs better recently and decided to share my (hopefully nicely) commented &lt;code&gt;init.el&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Why? Because it took me some time to find in the Emacs docs what I wanted. A readable, well commented config file would have helped so I'm sharing mine. Hopefully it'll also make it easy for you to just pickup the &lt;em&gt;things&lt;/em&gt; you like most. This config surely won't make everyone happy but, given that it's a github &lt;a class="reference external" href="http://gist.github.com/"&gt;gist&lt;/a&gt;, you're free to fork it or to add some comments providing feedback/suggestiond. Actually, feedback is very welcomed but please, keep your &lt;code&gt;Lisp&lt;/code&gt; easy to read and clean, this is for &lt;strong&gt;beginners&lt;/strong&gt;. Not to mention that I very much care about startup times!&lt;/p&gt;
&lt;script src="https://gist.github.com/giulivo/5396858.js"&gt;&lt;/script&gt;</content><category term="emacs"></category><category term="fedoraplanet"></category></entry><entry><title>Deploy OpenStack Heat on RHEL (and derivates)</title><link href="http://giuliofidente.com/2013/04/deploy-openstack-heat-on-rhel-and-derivates.html" rel="alternate"></link><published>2013-04-16T11:00:00+02:00</published><updated>2013-04-16T11:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-16:/2013/04/deploy-openstack-heat-on-rhel-and-derivates.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE (June 2013):&lt;/strong&gt; this post has been &lt;a class="reference external" href="http://openstack.redhat.com/Deploy_Heat_and_launch_your_first_Application"&gt;published on the RDO site&lt;/a&gt; and is now maintained there.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; provides orchestration of composite cloud applications using the CloudFormation API and templates; it is an incubated project of OpenStack. Its development cycle is to be Integrated in Havana and follow the full OpenStack release process. I want to go trough the steps needed to install and configure it as the &lt;a class="reference external" href="http://docs.openstack.org"&gt;official documentation&lt;/a&gt; is still scarce on the â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;UPDATE (June 2013):&lt;/strong&gt; this post has been &lt;a class="reference external" href="http://openstack.redhat.com/Deploy_Heat_and_launch_your_first_Application"&gt;published on the RDO site&lt;/a&gt; and is now maintained there.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; provides orchestration of composite cloud applications using the CloudFormation API and templates; it is an incubated project of OpenStack. Its development cycle is to be Integrated in Havana and follow the full OpenStack release process. I want to go trough the steps needed to install and configure it as the &lt;a class="reference external" href="http://docs.openstack.org"&gt;official documentation&lt;/a&gt; is still scarce on the matter. Firstly, what it does?&lt;/p&gt;
&lt;blockquote&gt;
Heat is a service to orchestrate multiple composite cloud applications using the AWS CloudFormation template format, through both an OpenStack-native ReST API and a CloudFormation-compatible Query API.&lt;/blockquote&gt;
&lt;p&gt;So you're going to deploy a composite application (made up of more than a single instance) on the cloud infrastructure, this also involves launchtime customizations of the VMs but, before start, some assumptions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;I'm using CentOS 6.4 / MySQL&lt;/li&gt;
&lt;li&gt;I'm using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository to install the packages&lt;/li&gt;
&lt;li&gt;The core &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; infrastructure is already configured and in good shape&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you don't have a working &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; deployment yet, I recommend you to follow the instructions on the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; site, you'll get one up and running in minutes by using &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;. When that is finished, start by installing the required packages for &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; to work:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-heat-*
&lt;/pre&gt;
&lt;p&gt;You'll get four new services installed: an engine, a native api, a cloudformation compatible api, a cloudwatch compatible api. You don't have to deploy them all on a single host but for the purpose of this guide it will be fine to do so.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="configuration"&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; comes with a script which creates (and populates) the needed database for it to work but you need to know your MySQL's &lt;code&gt;root&lt;/code&gt; account password. If you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;, than that is saved as &lt;code&gt;CONFIG_MYSQL_PW&lt;/code&gt; in the answers file (&lt;code&gt;/root/packstack-answers*&lt;/code&gt; by default). Now run the prepare script:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-db-setup rpm -y -r ${MYSQL_ROOT_PASSWORD} -p ${HEAT_DB_PASSWORD_OF_CHOICE}
&lt;/pre&gt;
&lt;p&gt;Check in &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; that your database connection string is correct:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection = mysql://heat:${HEAT_DB_PASSWORD}&amp;#64;localhost/heat
&lt;/pre&gt;
&lt;p&gt;Now go trough the &lt;em&gt;usual&lt;/em&gt; steps needed to create a new user, service and endpoint with Keystone and don't forget to source the admin credentials before starting (which are in &lt;code&gt;/root/keystonerc_admin&lt;/code&gt; if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone user-create --name heat --pass ${HEAT_USER_PASSWORD_OF_CHOICE}
# keystone user-role-add --user heat --role admin --tenant ${SERVICES_TENANT_NAME}
# keystone service-create --name heat --type orchestration
# keystone service-create --name heat-cfn --type cloudformation
# keystone endpoint-create --region RegionOne --service-id ${HEAT_CFN_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --adminurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --internalurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot;
# keystone endpoint-create --region RegionOne --service-id ${HEAT_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot; --adminurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s --internalurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Update the paste files at &lt;code&gt;/etc/heat/heat-api{,-cfn,-cloudwatch}-paste.ini&lt;/code&gt; with the credentials just created:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
admin_tenant_name = ${SERVICES_TENANT_NAME}
admin_user = heat
admin_password = ${HEAT_USER_PASSWORD}
&lt;/pre&gt;
&lt;p&gt;In there you also need to make sure that the following variables are pointing to your Keystone host (127.0.0.1 should just work if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt; as Keystone is probably installed on the same host):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
service_host = ${KEYSTONE_HOSTNAME}
auth_host = ${KEYSTONE_HOSTNAME}
auth_uri = http://${KEYSTONE_HOSTNAME}:35357/v2.0
keystone_ec2_uri = http://${KEYSTONE_HOSTNAME}:5000/v2.0/ec2tokens
&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; you've to make instead sure that the following variables &lt;strong&gt;do not&lt;/strong&gt; point to 127.0.0.1 even though the services are actually hosted on the same system because URLs will be passed over to the VMs, which don't have them available locally:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
heat_metadata_server_url = http://${HEAT_CFN_HOSTNAME}:8000
heat_waitcondition_server_url = http://${HEAT_CFN_HOSTNAME}:8000/v1/waitcondition
heat_watch_server_url = http://${HEAT_CLOUDWATCH_HOSTNAME}:8003
&lt;/pre&gt;
&lt;p&gt;The application templates can use wait conditions and signaling for the orchestration, &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; needs to create special users to receive the progress data and these users are, by default, given the role of &lt;code&gt;heat_stack_user&lt;/code&gt;. You can configure the role name in &lt;code&gt;heat-engine.conf&lt;/code&gt; or just create a so called role:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone role-create --name heat_stack_user
&lt;/pre&gt;
&lt;p&gt;The configuration should now be complete and the services can be started:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cd /etc/init.d &amp;amp;&amp;amp; for s in $(ls openstack-heat-*); do chkconfig $s on &amp;amp;&amp;amp; service $s start; done
&lt;/pre&gt;
&lt;p&gt;Make sure by checking the logs that everything was started successfully. Specifically, in case the engine service reports &lt;code&gt;ImportError: cannot import name Random&lt;/code&gt; then you're probably using an old version of &lt;code&gt;pycrypto&lt;/code&gt;. A fix has been merged upstream to workaround the issue. It's &lt;a class="reference external" href="https://review.openstack.org/#/c/26759/"&gt;a trivial change&lt;/a&gt; which you can apply manually to &lt;code&gt;heat/common/crypt.py&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="get-the-demo-files"&gt;
&lt;h2&gt;Get the demo files&lt;/h2&gt;
&lt;p&gt;It is time now to launch your first multi-instance cloud application! There are a number of sample templates available in the &lt;a class="reference external" href="https://github.com/openstack/heat"&gt;github repo&lt;/a&gt;, download the composed Wordpress example with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# wget https://raw.github.com/openstack/heat-templates/master/cfn/WordPress_Composed_Instances.template
&lt;/pre&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; can use the templates distributed for &lt;a class="reference external" href="http://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt;. These expect you to have a well known set of flavor types defined while the default flavors available in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; don't match strictly such a collection. To avoid the need of hack the templates, you can use an helpful script which recreates in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; the same flavors from AWS:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# curl https://raw.github.com/openstack/heat/master/tools/nova_create_flavors.sh | bash
&lt;/pre&gt;
&lt;p&gt;Every template also provides you with a list of usable distros and map these into an AMI string, for each arch. You will have to populate Glance with an image matching the AMI string that the template file is expecting to find.&lt;/p&gt;
&lt;p&gt;There is a tool, called &lt;a class="reference external" href="https://github.com/sdake/heat-jeos"&gt;heat-jeos&lt;/a&gt;, which can be used to create the JEOS images and upload them to Glance but there is also a collection of prebuilt images at: &lt;a class="reference external" href="http://fedorapeople.org/groups/heat/prebuilt-jeos-images/"&gt;http://fedorapeople.org/groups/heat/prebuilt-jeos-images/&lt;/a&gt; so I suggest you to just download one from &lt;code&gt;F17-x86_64-cfntools.qcow2&lt;/code&gt; or &lt;code&gt;U10-x86_64-cfntools.qcow2&lt;/code&gt; (which are referred by many if not all the templates available in the Heat's repo). To upload the F17 x86_64 image in Glance:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# glance image-create --name F17-x86_64-cfntools --disk-format qcow2 --container-format bare --is-public True --copy-from http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2
&lt;/pre&gt;
&lt;p&gt;While that is downloading, create a new keypair or upload you public key in nova to make sure you'll be able to login on the VMs using SSH:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# nova keypair-add --pub_key ~/.ssh/id_rsa.pub userkey
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="launch"&gt;
&lt;h2&gt;Launch!&lt;/h2&gt;
&lt;p&gt;It is time for the real fun now, launch your first composed application with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn create wordpress --template-file=WordPress_Composed_Instances.template --parameters=&amp;quot;DBUsername=wp;DBPassword=wp;KeyName=userkey;LinuxDistribution=F17&amp;quot;
&lt;/pre&gt;
&lt;p&gt;More parameters could have passed, note for instance the LinuxDistribution parameter discussed above. Now the interesting stuff:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn list
# heat-cfn event-list wordpress
&lt;/pre&gt;
&lt;p&gt;After the VMs are launched, the mysql/httpd/wordpress installation and configuration begins, the process is driven by the &lt;code&gt;cfntools&lt;/code&gt;, installed in the VMs images. It will take quite some time, despite the &lt;code&gt;event-list&lt;/code&gt; reporting completion for the WordPress install too early (there is signaling, via &lt;code&gt;cfn-signal&lt;/code&gt;, only in the MySQL template). You can login on the instances and check the logs or just use &lt;code&gt;ps&lt;/code&gt; to see how things are going. After some minutes the setup should be finished:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn describe wordpress
# wget ${WebsiteURL} // that is an URL from the previous command!
&lt;/pre&gt;
&lt;p&gt;If anything goes wrong, check the logs at &lt;code&gt;/var/log/heat/engine.log&lt;/code&gt; or look at the scripts passed as &lt;code&gt;UserData&lt;/code&gt; to the instances, these should be found in &lt;code&gt;/var/lib/cloud/data/&lt;/code&gt;. Time to hack your very own template and delete the test deployment! :)&lt;/p&gt;
&lt;/div&gt;
</content><category term="openstack"></category><category term="heat"></category><category term="fedoraplanet"></category><category term="rhel"></category><category term="centos"></category></entry><entry><title>1366x768 is not allowed in the EDID block. Here's how to write your XOrg modeline.</title><link href="http://giuliofidente.com/2012/10/1366x768-is-not-allowed-in-the-edid-block-heres-how-to-write-your-xorg-modeline.html" rel="alternate"></link><published>2012-10-09T00:00:00+02:00</published><updated>2012-10-09T00:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2012-10-09:/2012/10/1366x768-is-not-allowed-in-the-edid-block-heres-how-to-write-your-xorg-modeline.html</id><summary type="html">&lt;p&gt;Looks like there are many LCD panels/TVs out there with a native resolution of 1366x768. That is indeed a very close approximation to the expected 16:9 rectangle, except XOrg keeps showing you a resolution of 1360x768 (or 1368x768) instead of the native 1366x768. Why? Because 1366 is not divisible by 8 and that's not valid in an EDID block. &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Extended_display_identification_data#Limitations"&gt;Learn more on wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You need a custom &lt;a class="reference external" href="http://en.wikipedia.org/wiki/XFree86_Modeline"&gt;modeline in your xorg.conf&lt;/a&gt; file â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Looks like there are many LCD panels/TVs out there with a native resolution of 1366x768. That is indeed a very close approximation to the expected 16:9 rectangle, except XOrg keeps showing you a resolution of 1360x768 (or 1368x768) instead of the native 1366x768. Why? Because 1366 is not divisible by 8 and that's not valid in an EDID block. &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Extended_display_identification_data#Limitations"&gt;Learn more on wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You need a custom &lt;a class="reference external" href="http://en.wikipedia.org/wiki/XFree86_Modeline"&gt;modeline in your xorg.conf&lt;/a&gt; file for that. The NVIDIA drivers also have some &lt;a class="reference external" href="http://us.download.nvidia.com/XFree86/Linux-x86_64/304.43/README/xconfigoptions.html"&gt;ModeValidation&lt;/a&gt; setting which needs some attention. Let's start with the last one, you need to add the following (in the Screen section):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Option &amp;quot;ModeValidation&amp;quot; &amp;quot;AllowNonEdidModes, NoWidthAlignmentCheck&amp;quot;
&lt;/pre&gt;
&lt;p&gt;To make XOrg log files more verbose, you may also want to add the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Option &amp;quot;ModeDebug&amp;quot; &amp;quot;True&amp;quot;
&lt;/pre&gt;
&lt;p&gt;And what about the modeline? So &lt;a class="reference external" href="http://howto-pages.org/ModeLines/"&gt;this guy wrote a lot about it&lt;/a&gt; but for our purposes, let's just start with &lt;a class="reference external" href="http://www.xfree86.org/current/xvidtune.1.html"&gt;xvidtune&lt;/a&gt; to check for the current settings:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ xvidtune -show
&amp;quot;1360x768&amp;quot; 84.75 1360 1432 1568 1776 768 771 776 798 -hsync +vsync
&lt;/pre&gt;
&lt;p&gt;After the modeline 'description' (whis is 1360x768), the first number you see represents the pixel clock speed. The remaining eight numbers are two groups of four numbers intended to set the horizontal and vertical resolution. The interesting thing is that you can get your hrefresh and vrefresh value with a simple formula: &lt;code&gt;hrefresh = 84.75/1776&lt;/code&gt; and &lt;code&gt;vrefresh = 84.75/(1776*798)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now, on my LCD manual I had the following valuable informations:&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;1360*768&lt;/dt&gt;
&lt;dd&gt;47.72 59.8 84.75&lt;/dd&gt;
&lt;dt&gt;1366*768&lt;/dt&gt;
&lt;dd&gt;47.56 59.6 84.75&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Those are hrefresh, vrefresh and pixel clock. Let's put those in the modeline adding some more small changes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;quot;1366x768&amp;quot; 84.75 1366 1438 1574 1782 768 771 776 798 -hsync +vsync
&lt;/pre&gt;
&lt;p&gt;Now here is how the new numbers were found:&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;84.75&lt;/dt&gt;
&lt;dd&gt;is the pixel clock and remains the same as per service manual&lt;/dd&gt;
&lt;dt&gt;1360 became 1366&lt;/dt&gt;
&lt;dd&gt;that's the hresolution we want&lt;/dd&gt;
&lt;dt&gt;1776 became 1782&lt;/dt&gt;
&lt;dd&gt;that is what we need to get the 47.56 hrefresh and 59.6 vrefresh values indicated in the service manual, check yourself with the formula &lt;code&gt;84.75/1782 ~= 47.56&lt;/code&gt; and &lt;code&gt;84.75/(1782\*798) ~= 59.6&lt;/code&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Also, from the initial xvidtune output, the numbers 1432 and 1568 represent some delay, measured in pixels pictured past the viewable area, at the defined pixel clock speed. They are used to set &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Analog_television"&gt;front porch, sync pulse and back porch&lt;/a&gt;; by using 1360 as the number of horizontal pixels you get the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
(1432-1360)/84.75 = 0.84us black on right side
(1568-1432)/84.75 = 1.60us sync pulse width
(1776-1568)/84.75 = 2.54us black on left side
&lt;/pre&gt;
&lt;p&gt;Now use 1366 as the number of horizontal pixels and keep the delay unchanged:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
(1438-1366)/84.75 = 0.84us black on right side
(1574-1438)/84.75 = 1.60us sync pulse width
(1782-1574)/84.75 = 2.54us black on left side
&lt;/pre&gt;
&lt;p&gt;Finally, for this blog post all the values I'm using refer to the LG 37LG3000. Enjoy!&lt;/p&gt;
</content><category term="1366x768"></category><category term="xorg modeline"></category><category term="linux"></category><category term="fedoraplanet"></category><category term="nvidia"></category></entry><entry><title>My attempts at MapReduce using MongoDB</title><link href="http://giuliofidente.com/2012/06/my-attempts-at-mapreduce-using-mongodb.html" rel="alternate"></link><published>2012-06-14T00:00:00+02:00</published><updated>2012-06-14T00:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2012-06-14:/2012/06/my-attempts-at-mapreduce-using-mongodb.html</id><summary type="html">&lt;p&gt;I was sorting a tree in my (python) webapp instead of having the database to do it for me. This is how I moved it back to the database by using a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/MapReduce"&gt;MapReduce&lt;/a&gt; job. I had a collection structured like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;feed_oid&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd268d2ab87b2d8927d7eee&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;title&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;blah blah&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;updated&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1339702524&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;watchers&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd276fc66224c1ee8000006&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Such a collection is called &lt;code&gt;articles&lt;/code&gt; and in there I get a document for every article published by an rss feed. &lt;code&gt;feed_oid&lt;/code&gt; is â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was sorting a tree in my (python) webapp instead of having the database to do it for me. This is how I moved it back to the database by using a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/MapReduce"&gt;MapReduce&lt;/a&gt; job. I had a collection structured like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;feed_oid&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd268d2ab87b2d8927d7eee&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;title&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;blah blah&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;updated&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1339702524&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;watchers&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd276fc66224c1ee8000006&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Such a collection is called &lt;code&gt;articles&lt;/code&gt; and in there I get a document for every article published by an rss feed. &lt;code&gt;feed_oid&lt;/code&gt; is an identifier I assign to every rss feed that I'm crawling and &lt;code&gt;watchers&lt;/code&gt; contains a list of identifiers assigned to the people voting on such an article.&lt;/p&gt;
&lt;p&gt;I wanted to find out the number of times a particular watcher appeared in the full list of articles and than, group that by the rss feed, so that I could end up with the number of times a person voted on articles published by the same feed.&lt;/p&gt;
&lt;p&gt;The following are my map and reduce functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;emit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;feed_oid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;oids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nx"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The map function is called for every object matching the query filter, so it gets access to 'this'. The reduce function receives an array of values (all set to 1, by my map function) for every feed_oid emitted.&lt;/p&gt;
&lt;p&gt;Here is how I spawn the MapReduce job (querying by the watcher id):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;articles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mapReduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;watchers&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd276fc66224c1ee8000006&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nx"&gt;out&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mapreduceout&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mapreduceout&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd268d2ab87b2d8927d7eea&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4fd268d2ab87b2d8927d7eee&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looks like this guy voted 4 times on articles appeared on the feed 4fd268d2ab87b2d8927d7eee and 1 4fd268d2ab87b2d8927d7eea :P&lt;/p&gt;
</content><category term="mapreduce"></category><category term="fedoraplanet"></category><category term="mongodb"></category></entry><entry><title>YUM history (!)</title><link href="http://giuliofidente.com/2012/04/yum-history.html" rel="alternate"></link><published>2012-04-26T00:00:00+02:00</published><updated>2012-04-26T00:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2012-04-26:/2012/04/yum-history.html</id><summary type="html">&lt;p&gt;While browsing the &lt;a class="reference external" href="http://yum.baseurl.org/"&gt;YUM&lt;/a&gt; man page for some details about the query command I happened to find one of my most wanted feature in a package manager! YUM has some &lt;a class="reference external" href="http://docs.fedoraproject.org/en-US/Fedora/16/html/System_Administrators_Guide/sec-Yum-Transaction_History.html"&gt;history&lt;/a&gt; command which allows for investigation of past transactions and even &lt;strong&gt;undo&lt;/strong&gt; or &lt;strong&gt;rollback&lt;/strong&gt; actions. Epic. I frequently find myself going through install/uninstall steps which not only mess around but I tend to forget about the installed and now unneeded deps.&lt;/p&gt;
&lt;p&gt;I'll go through â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;While browsing the &lt;a class="reference external" href="http://yum.baseurl.org/"&gt;YUM&lt;/a&gt; man page for some details about the query command I happened to find one of my most wanted feature in a package manager! YUM has some &lt;a class="reference external" href="http://docs.fedoraproject.org/en-US/Fedora/16/html/System_Administrators_Guide/sec-Yum-Transaction_History.html"&gt;history&lt;/a&gt; command which allows for investigation of past transactions and even &lt;strong&gt;undo&lt;/strong&gt; or &lt;strong&gt;rollback&lt;/strong&gt; actions. Epic. I frequently find myself going through install/uninstall steps which not only mess around but I tend to forget about the installed and now unneeded deps.&lt;/p&gt;
&lt;p&gt;I'll go through a basic &lt;code&gt;history&lt;/code&gt; usage example but there is a lot more to discover. Consider the following command:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install anjuta
...
Installed:
 anjuta.i686 1:3.2.0-1.fc16

Dependency Installed:
 apr.i686 0:1.4.6-1.fc16
 apr-util.i686 0:1.3.12-1.fc16
 autogen.i686 0:5.9.4-8.fc15
 autogen-libopts.i686 0:5.9.4-8.fc15
 devhelp.i686 1:3.2.0-1.fc16
 glade3-libgladeui.i686 1:3.10.0-6.fc16
 guile.i686 5:1.8.8-3.fc16
 libgda.i686 1:4.2.8-2.fc16
 libgda-sqlite.i686 1:4.2.8-2.fc16
 libgdl.i686 1:3.2.0-1.fc16
 sqlite-devel.i686 0:3.7.7.1-1.fc16
 subversion-libs.i686 0:1.6.17-5.fc16
 vala.i686 0:0.14.2-3.fc16
&lt;/pre&gt;
&lt;p&gt;Many dependencies have been installed and you surely won't remember all of them when later removing anjuta. You could go through some cleaning session using &lt;code&gt;package-cleanup&lt;/code&gt;, from
&lt;a class="reference external" href="http://yum.baseurl.org/wiki/YumUtils"&gt;yum-utils&lt;/a&gt; but that isn't really intended to revert back your system status, it will just help you remove unneeded packages. Here's instead what &lt;code&gt;history&lt;/code&gt; can do for you:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum history list anjuta
Loaded plugins: downloadonly, langpacks, presto, refresh-packagekit
ID   | Command line       | Date and time    | Action(s)  | Altered
-------------------------------------------------------------------
 172 | install anjuta     | 2012-04-26 09:02 | Install    |   14

# yum history info 172
Loaded plugins: downloadonly, langpacks, presto, refresh-packagekit
Transaction ID : 172
Begin time     : Thu Apr 26 09:02:57 2012
Begin rpmdb    : 1225:459cfe1ee50fe38d585386f265e6647ab8d4b5a9
End time       :            09:03:19 2012 (22 seconds)
End rpmdb      : 1239:9784f29b6dff78d982e401bfa5e4cbd9620c47ed
User           : Giulio Fidente
Return-Code    : Success
Command Line   : install anjuta
Transaction performed with:
    Installed     rpm-4.9.1.3-1.fc16.i686               &amp;#64;updates
    Installed     yum-3.4.3-23.fc16.noarch              &amp;#64;updates
    Installed     yum-metadata-parser-1.1.4-5.fc16.i686 &amp;#64;koji-overrides
Packages Altered:
    Install     anjuta-1:3.2.0-1.fc16.i686             &amp;#64;fedora
    Dep-Install apr-1.4.6-1.fc16.i686                  &amp;#64;updates
    Dep-Install apr-util-1.3.12-1.fc16.i686            &amp;#64;fedora
    Dep-Install autogen-5.9.4-8.fc15.i686              &amp;#64;fedora
    Dep-Install autogen-libopts-5.9.4-8.fc15.i686      &amp;#64;fedora
    Dep-Install devhelp-1:3.2.0-1.fc16.i686            &amp;#64;fedora
    Dep-Install glade3-libgladeui-1:3.10.0-6.fc16.i686 &amp;#64;updates
    Dep-Install guile-5:1.8.8-3.fc16.i686              &amp;#64;fedora
    Dep-Install libgda-1:4.2.8-2.fc16.i686             &amp;#64;updates
    Dep-Install libgda-sqlite-1:4.2.8-2.fc16.i686      &amp;#64;updates
    Dep-Install libgdl-1:3.2.0-1.fc16.i686             &amp;#64;fedora
    Dep-Install sqlite-devel-3.7.7.1-1.fc16.i686       &amp;#64;fedora
    Dep-Install subversion-libs-1.6.17-5.fc16.i686     &amp;#64;fedora
    Dep-Install vala-0.14.2-3.fc16.i686                &amp;#64;updates

# yum history undo 172
...
Removed:
 anjuta.i686 1:3.2.0-1.fc16
 apr.i686 0:1.4.6-1.fc16
 apr-util.i686 0:1.3.12-1.fc16
 autogen.i686 0:5.9.4-8.fc15
 autogen-libopts.i686 0:5.9.4-8.fc15
 devhelp.i686 1:3.2.0-1.fc16
 glade3-libgladeui.i686 1:3.10.0-6.f16
 guile.i686 5:1.8.8-3.fc16
 libgda.i686 1:4.2.8-2.fc16
 libgda-sqlite.i686 1:4.2.8-2.fc16
 libgdl.i686 1:3.2.0-1.fc16
 sqlite-devel.i686 0:3.7.7.1-1.fc16
 subversion-libs.i686 0:1.6.17-5.fc16
 vala.i686 0:0.14.2-3.fc16
&lt;/pre&gt;
&lt;p&gt;Great isn't it? And there is a lot more! The &lt;code&gt;rollback&lt;/code&gt; command will revert back the status of the &lt;strong&gt;whole&lt;/strong&gt; software packages installed at the time of the transaction ID.&lt;/p&gt;
</content><category term="fedora"></category><category term="fedoraplanet"></category><category term="yum"></category></entry><entry><title>OS X Network Install using Linux (updates)</title><link href="http://giuliofidente.com/2009/01/os-x-network-install-using-linux-updates.html" rel="alternate"></link><published>2009-01-06T00:00:00+01:00</published><updated>2009-01-06T00:00:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2009-01-06:/2009/01/os-x-network-install-using-linux-updates.html</id><summary type="html">&lt;p&gt;Do you still remember &lt;a class="reference external" href="http://giuliofidente.com/2006/11/os-x-network-install-using-linux.html"&gt;this&lt;/a&gt;? It was a good post about the OS X install via the network using a GNU/Linux install server. I went back to read and use it after a few days to install the version 10.5 (leopard) of OS X and it worked well but there's a couple of things missing in that post which I'd like to share here.&lt;/p&gt;
&lt;p&gt;The problems were mainly in mounting the leopard disc â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Do you still remember &lt;a class="reference external" href="http://giuliofidente.com/2006/11/os-x-network-install-using-linux.html"&gt;this&lt;/a&gt;? It was a good post about the OS X install via the network using a GNU/Linux install server. I went back to read and use it after a few days to install the version 10.5 (leopard) of OS X and it worked well but there's a couple of things missing in that post which I'd like to share here.&lt;/p&gt;
&lt;p&gt;The problems were mainly in mounting the leopard disc. If you try to do that on a GNU/Linux system you should only see some files about bootcamp, it is indeed a double format dvd which includes two sections, one is iso9660 formatted and another is hfs+ formatted. To find the files I mentioned, you'll have to mount the hfs+ formatted section ... which is hidden but you can find it using this very helpful link: &lt;a class="reference external" href="http://www.64lines.com/mounting-hfs-plus"&gt;Mounting HFS+ Hybrid Disks on Linux&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, because of the additional steps, when you're at this:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
dd if=/dev/hdc of=/tftpboot/macosx.img
&lt;/pre&gt;
&lt;p&gt;you'll have to replace /dev/hdc with the /dev/loop0 device created by the instructions linked before.&lt;/p&gt;
</content><category term="osx network install"></category><category term="linux network install"></category><category term="leopard network install"></category><category term="fedoraplanet"></category></entry><entry><title>OS X Network Install using Linux</title><link href="http://giuliofidente.com/2006/11/os-x-network-install-using-linux.html" rel="alternate"></link><published>2006-11-01T00:00:00+01:00</published><updated>2006-11-01T00:00:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2006-11-01:/2006/11/os-x-network-install-using-linux.html</id><summary type="html">&lt;p&gt;The title says it all. We're going to install an OS X client via network using a GNU/Linux box as DHCP/TFTP/NFS server.&lt;/p&gt;
&lt;p&gt;First you'll want to setup your DHCP, TFTP and NFS server.&lt;/p&gt;
&lt;p&gt;The default location for the TFTP server root on my system was &lt;code&gt;/tftpboot&lt;/code&gt;. It may be different on other distro so change at will. This directory is where we're going to put all the important files. Three files come â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The title says it all. We're going to install an OS X client via network using a GNU/Linux box as DHCP/TFTP/NFS server.&lt;/p&gt;
&lt;p&gt;First you'll want to setup your DHCP, TFTP and NFS server.&lt;/p&gt;
&lt;p&gt;The default location for the TFTP server root on my system was &lt;code&gt;/tftpboot&lt;/code&gt;. It may be different on other distro so change at will. This directory is where we're going to put all the important files. Three files come from the OS X disc (although you'll have to rename two of them) and the fourth is a simple image of the OS X disc itself.&lt;/p&gt;
&lt;p&gt;Mount the Mac OS X disc and copy and rename the following files into your TFTP server root&lt;/p&gt;
&lt;pre class="literal-block"&gt;
cp /cdrom/System/Library/CoreServices/BootX /tftpboot/BootX
cp /cdrom/mach\_kernel /tftpboot/mach.macosx
cp /cdrom/System/Library/Extensions.mkext /tftpboot/mach.macosx.mkext
&lt;/pre&gt;
&lt;p&gt;Unmount and make an image of the install disc in the TFTP server root&lt;/p&gt;
&lt;pre class="literal-block"&gt;
dd if=/dev/hdc of=/tftpboot/macosx.img
&lt;/pre&gt;
&lt;p&gt;On your NFS server, you'll want to modify &lt;code&gt;/etc/exports&lt;/code&gt; to include something like the following&lt;/p&gt;
&lt;pre class="literal-block"&gt;
/tftpboot/ mac-ip-address(ro,insecure)
&lt;/pre&gt;
&lt;p&gt;where mac-ip-address is the mac address assigned to your mac manually (see step 7) or by the DHCP server.&lt;/p&gt;
&lt;p&gt;At this point you'll want to start the TFTP server and NFS services.&lt;/p&gt;
&lt;p&gt;Boot into the open firmware (by holding command+option+O+F) and issue the following commands&lt;/p&gt;
&lt;pre class="literal-block"&gt;
setenv boot-device enet:ip-address-of-linux-server,BootX
setenv boot-args rp=nfs:ip-address-of-linux-server:/tftpboot/:macosx.img
boot
&lt;/pre&gt;
&lt;p&gt;where ip-address-of-linux-server is... self-explanatory.&lt;/p&gt;
&lt;p&gt;The well familiar Mac boot sequence should start except now you have a little spinning world as logo while it tries to make a connection to the Linux server. You'll probably want to hold command+V while booting the Mac to see what's actually happening and to ensure the whole process is going smoothly.&lt;/p&gt;
&lt;p&gt;I hope it helped!&lt;/p&gt;
</content><category term="osx network install"></category><category term="linux network install"></category><category term="leopard network install"></category><category term="fedoraplanet"></category></entry></feed>