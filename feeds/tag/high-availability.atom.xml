<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Giulio Fidente</title><link href="http://giuliofidente.com/" rel="alternate"></link><link href="http://giuliofidente.com/feeds/tag/high-availability.atom.xml" rel="self"></link><id>http://giuliofidente.com/</id><updated>2014-08-21T15:02:00+02:00</updated><entry><title>TripleO vs OpenStack HA</title><link href="http://giuliofidente.com/2014/08/tripleo-vs-openstack-ha.html" rel="alternate"></link><updated>2014-08-21T15:02:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2014-08-21:2014/08/tripleo-vs-openstack-ha.html</id><summary type="html">&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite the OpenStack services being designed around a scale-out concept, while attempting to achieve that in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we spotted a number of weak angles, some of which could be worked around, others instead still need some changes in the core service. You're encouraged to try what we have available today and help with the rest.&lt;/p&gt;
&lt;p&gt;So to try out OpenStack HA with &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; you just set a number &amp;gt;= 3 for &lt;code&gt;OVERCLOUD_CONTROLSCALE&lt;/code&gt; and continue with &lt;a class="reference external" href="http://docs.openstack.org/developer/tripleo-incubator/devtest.html"&gt;devtest&lt;/a&gt; as usual. Nodes will be configured appropriately:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export OVERCLOUD_CONTROLSCALE=3
source scripts/devtest_variables.sh
...
&lt;/pre&gt;
&lt;p&gt;Don't forget this is only tested on a few distros for now, I'd pick some Fedora 20.&lt;/p&gt;
&lt;p&gt;On the controller nodes, MariaDB with Galera (for Fedora) is going to provide for a reliable SQL. There is still some work in progress to make sure the Galera cluster can be restarted correctly should all the controllers go down at the same time but, for single node failures, this should be safe to use.&lt;/p&gt;
&lt;p&gt;RabbitMQ nodes are clustered and balanced (via HAProxy), queues replicated.&lt;/p&gt;
&lt;p&gt;And with regards to the OpenStack services, these are configured in a balancing manner (again, using HAProxy) except for those cases where this wouldn't have worked, notably the Neutron L3 agent and the Ceilometer Central agent, yet these are under control via Pacemaker and a single instance is expected to be running at all times. Cinder instead remains uncovered as volumes would require a shared storage for proper HA. A spec has been proposed for this though.&lt;/p&gt;
&lt;p&gt;Also, behind the scenes, the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; template language addon shipped as &lt;em&gt;merge.py&lt;/em&gt; and included in &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates"&gt;tripleo-heat-templates&lt;/a&gt;, which allows for example for scaling of the resources definition, is currently going to be removed and replaced with code living entirely in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And there is more so once you tried, join us on #tripleo &amp;#64; freenode for the real fun!&lt;/p&gt;
</summary><category term="openstack"></category><category term="tripleo"></category><category term="high availability"></category><category term="meetup"></category><category term="fedoraplanet"></category></entry></feed>