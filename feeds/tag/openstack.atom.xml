<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Giulio Fidente (aka Giulivo Navigante)</title><link href="http://giuliofidente.com/" rel="alternate"></link><link href="http://giuliofidente.com/feeds/tag/openstack.atom.xml" rel="self"></link><id>http://giuliofidente.com/</id><updated>2013-06-27T02:45:00+02:00</updated><entry><title>OpenStack Glance - Use Swift as backend</title><link href="http://giuliofidente.com/2013/06/openstack-glance-use-swift-as-backend.html" rel="alternate"></link><updated>2013-06-27T02:45:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-27:2013/06/openstack-glance-use-swift-as-backend.html</id><summary type="html">&lt;p&gt;On &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; again but about &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; this time. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; is the component in charge of hosting the images (and image snapshots) to be cloned for the ephemeral instances. Images usually are just some random big files so it makes perfect sense to use &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt; for such an object (a File Object storage)!&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; image node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This said, only few changes are needed to swap from local filesystem storage to &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;. Edit the &lt;code&gt;glance-api.conf&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
default_store = swift
swift_store_auth_address = $KEYSTONE_SERVICE_PROTOCOL://$KEYSTONE_SERVICE_HOST:$KEYSTONE_SERVICE_PORT/v2.0/
swift_store_user = $SERVICE_TENANT_NAME:glance
swift_store_key = $SERVICE_PASSWORD
swift_store_create_container_on_put = True
&lt;/pre&gt;
&lt;p&gt;These are probably self-explanatory but I have a few tips to spare! If you decide to go via https for the keystone service, make sure you can validate locally (on &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt;) the certificate. If unsure about the values for the $SERVICE_* variables, those should have been set already in the &lt;code&gt;keystone_authtoken&lt;/code&gt; section.&lt;/p&gt;
&lt;p&gt;Short and straight to the point!&lt;/p&gt;
</summary><category term="openstack"></category><category term="glance"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Configure multiple backends</title><link href="http://giuliofidente.com/2013/06/openstack-cinder-configure-multiple-backends.html" rel="alternate"></link><updated>2013-06-16T16:37:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-16:2013/06/openstack-cinder-configure-multiple-backends.html</id><summary type="html">&lt;p&gt;Following &lt;a class="reference external" href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html"&gt;my first post of the series&lt;/a&gt; discussing how to scale &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; to multiple nodes, with this I want to approach the configuration and usage of the multibackend feature landed in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; with the Grizzly release.&lt;/p&gt;
&lt;p&gt;This feature allows you to configure a single volume node for use with more than a single backend driver. You can find all about the few configuration bits needed also in the &lt;a class="reference external" href="http://docs.openstack.org/trunk/openstack-block-storage/admin/content/multi_backend.html"&gt;OpenStack block storage documentation&lt;/a&gt;. That makes this post somehow redundant but I wanted to keep up with the series and the topic is well worth to be kept also here.&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; volume node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assuming we want our node, configured with some LVM based and an additional NFS based backend, this is what we would need to add into &lt;code&gt;cinder.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
enabled_backends=lvm1,nfs1
[lvm1]
volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver
volume_backend_name=LVM_iSCSI
[nfs1]
nfs_shares_config=${PATH_TO_YOUR_SHARES_FILE}
volume_driver=cinder.volume.drivers.nfs.NfsDriver
volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;enabled_backends&lt;/code&gt; value defines some names (separated by a comma) for the config groups. These do not have to match the driver name nor the backend name.&lt;/p&gt;
&lt;p&gt;When the configuration is complete, to use a particular backend when allocating new volumes, you'll have to pass a &lt;code&gt;volume_type&lt;/code&gt; parameter to the creation command. Such a type has to be created beforehand and to have some backends assigned to it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder type-create lvm
# cinder type-key lvm set volume_backend_name=LVM_iSCSI
# cinder type-create nfs
# cinder type-key nfs set volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;Finally, to create your volumes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder create --volume_type lvm --display_name inlvm 1
&lt;/pre&gt;
&lt;p&gt;For people using the REST interface, to set any &lt;code&gt;type-key&lt;/code&gt; property, including &lt;code&gt;volume_backend_name&lt;/code&gt;, you pass that information along with the request as &lt;a class="reference external" href="https://github.com/openstack/cinder/blob/master/cinder/api/contrib/types_extra_specs.py"&gt;extra specs&lt;/a&gt;. You can list those indeed to make sure the configuration is working as expected:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
#  cinder extra-specs-list
&lt;/pre&gt;
&lt;p&gt;Note that you can have backends of the same type (driver) using different names (say two LVM based backends allocating volumes in different volume groups) or you can also have backends of the same type using the same name! The scheduler is in charge of making the proper decision on how to pickup the correct backend at creation time so a few notes on the filter scheduler (enabled by default in Grizzly):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;firstly it filters the available backends (AvailabilityZoneFilter, CapacityFilter and CapabilitiesFilter are enabled by default and the backend name is matched against the capabilities)&lt;/li&gt;
&lt;li&gt;secondly weights the previously filtered backends (CapacityWeigher is the only one enabled by default)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The CapacityWeigher attributes high score to backends with the most available space, so new volumes are allocated within the backend with the more space available matching the particular name in the request.&lt;/p&gt;
</summary><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Add more volume nodes</title><link href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html" rel="alternate"></link><updated>2013-04-30T02:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-30:2013/04/openstack-cinder-add-more-volume-nodes.html</id><summary type="html">&lt;p&gt;With this being the first of a short series, I'd like to publish some articles intendend to cover the required steps to configure &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; (&lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; block storage service) in a mid/large deployment scenario. The idea is to discuss at least three topics: how to scale the service by adding more volume nodes; how to ensure high-availablity for the API and Scheduler sub-services; leverage the multi-backend feature landed in Grizzly.&lt;/p&gt;
&lt;p&gt;I'm starting with this post on the scaling issue first. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; is composed of three main parts, the API server, the scheduler and the volume service. The volume service is some sort of abstraction layer between the API and the actual resources provider.&lt;/p&gt;
&lt;p&gt;By adding more volume nodes into the environment you will be able to increase the total offering of block storage to the tenants. Each volume node can either provide volumes by allocating them locally or on a remote container like an NFS or GlusterFS share.&lt;/p&gt;
&lt;p&gt;Some assumptions before getting into the practice:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general OpenStack architecture&lt;/li&gt;
&lt;li&gt;you have at least one Cinder node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to do on the candidate node is to install the required packages. I'm running the examples on CentOS and using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository which makes this step as simple as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-cinder
&lt;/pre&gt;
&lt;p&gt;If you plan to host new volumes using the locally available storage dont' forget to create a volume group called &lt;code&gt;cinder-volumes&lt;/code&gt; (the name can be configured via the &lt;code&gt;cinder_volume&lt;/code&gt; parameter). Also don't forget to configure the &lt;code&gt;tgtd&lt;/code&gt; to include the config files created dynamically by &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt;. Add a line like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
include /etc/cinder/volumes/*
&lt;/pre&gt;
&lt;p&gt;in your &lt;code&gt;/etc/tgt/targets.conf&lt;/code&gt; file. Now enable and start the &lt;code&gt;tgtd&lt;/code&gt; service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# chkconfig tgtd on
# service tgtd start
&lt;/pre&gt;
&lt;p&gt;Amongst the three init services installed by &lt;code&gt;openstack-cinder&lt;/code&gt; you only need to run &lt;code&gt;openstack-cinder-volume&lt;/code&gt;, which gets configured in &lt;code&gt;/etc/cinder/cinder.conf&lt;/code&gt;. Configure it to connect to the existing &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; database (the db in use by the pre-existing node) and to the existing AMQP broker (again, in use by the pre-existing node) by setting the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection=mysql://cinder:${CINDER_DB_PASSWORD}&amp;#64;${CINDER_DB_HOST}/cinder
qpid_hostname=${QPIDD_BROKER}
&lt;/pre&gt;
&lt;p&gt;Set the credentials if needed and/or change the &lt;code&gt;rpc_backend&lt;/code&gt; setting if you're not using &lt;a class="reference external" href="http://qpid.apache.org/"&gt;Qpid&lt;/a&gt; as your message broker. One more setting, not really required to change but worth checking if you're using the local resources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
iscsi_ip_address=${TGTD_IP_ADDRESS}
&lt;/pre&gt;
&lt;p&gt;That should match the public ip address of the volume node just installed. The iSCSI targets created locally using &lt;code&gt;tgtadm/tgtd&lt;/code&gt; have to be reachable by the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Nova"&gt;Nova&lt;/a&gt; nodes. The IP address of each target is stored in the database with every volume created. The &lt;code&gt;iscsi_ip_address&lt;/code&gt; prameter sets what is the IP address to be given to the initiators.&lt;/p&gt;
&lt;p&gt;At this point you should be ready to start the volume service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# service openstack-cinder-volume start
&lt;/pre&gt;
&lt;p&gt;Verify that it started by checking the logs (&lt;code&gt;/var/log/cinder/volume.log&lt;/code&gt;) or by issueing on any &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; node:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder-manage host list
&lt;/pre&gt;
&lt;p&gt;you should see all of your volume nodes listed. From now on you can create new volumes as usual and they will be allocated on any of the volume nodes, keep in mind that the scheduler will default to the node with the most space available.&lt;/p&gt;
</summary><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>Deploy OpenStack Heat on RHEL (and derivates)</title><link href="http://giuliofidente.com/2013/04/deploy-openstack-heat-on-rhel-and-derivates.html" rel="alternate"></link><updated>2013-04-16T11:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-16:2013/04/deploy-openstack-heat-on-rhel-and-derivates.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE (June 2013):&lt;/strong&gt; this post has been &lt;a class="reference external" href="http://openstack.redhat.com/Deploy_Heat_and_launch_your_first_Application"&gt;published on the RDO site&lt;/a&gt; and is now maintained there.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; provides orchestration of composite cloud applications using the CloudFormation API and templates; it is an incubated project of OpenStack. Its development cycle is to be Integrated in Havana and follow the full OpenStack release process. I want to go trough the steps needed to install and configure it as the &lt;a class="reference external" href="http://docs.openstack.org"&gt;official documentation&lt;/a&gt; is still scarce on the matter. Firstly, what it does?&lt;/p&gt;
&lt;blockquote&gt;
Heat is a service to orchestrate multiple composite cloud applications using the AWS CloudFormation template format, through both an OpenStack-native ReST API and a CloudFormation-compatible Query API.&lt;/blockquote&gt;
&lt;p&gt;So you're going to deploy a composite application (made up of more than a single instance) on the cloud infrastructure, this also involves launchtime customizations of the VMs but, before start, some assumptions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;I'm using CentOS 6.4 / MySQL&lt;/li&gt;
&lt;li&gt;I'm using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository to install the packages&lt;/li&gt;
&lt;li&gt;The core &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; infrastructure is already configured and in good shape&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you don't have a working &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; deployment yet, I recommend you to follow the instructions on the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; site, you'll get one up and running in minutes by using &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;. When that is finished, start by installing the required packages for &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; to work:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-heat-*
&lt;/pre&gt;
&lt;p&gt;You'll get four new services installed: an engine, a native api, a cloudformation compatible api, a cloudwatch compatible api. You don't have to deploy them all on a single host but for the purpose of this guide it will be fine to do so.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="configuration"&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; comes with a script which creates (and populates) the needed database for it to work but you need to know your MySQL's &lt;code&gt;root&lt;/code&gt; account password. If you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;, than that is saved as &lt;code&gt;CONFIG_MYSQL_PW&lt;/code&gt; in the answers file (&lt;code&gt;/root/packstack-answers*&lt;/code&gt; by default). Now run the prepare script:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-db-setup rpm -y -r ${MYSQL_ROOT_PASSWORD} -p ${HEAT_DB_PASSWORD_OF_CHOICE}
&lt;/pre&gt;
&lt;p&gt;Check in &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; that your database connection string is correct:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection = mysql://heat:${HEAT_DB_PASSWORD}&amp;#64;localhost/heat
&lt;/pre&gt;
&lt;p&gt;Now go trough the &lt;em&gt;usual&lt;/em&gt; steps needed to create a new user, service and endpoint with Keystone and don't forget to source the admin credentials before starting (which are in &lt;code&gt;/root/keystonerc_admin&lt;/code&gt; if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone user-create --name heat --pass ${HEAT_USER_PASSWORD_OF_CHOICE}
# keystone user-role-add --user heat --role admin --tenant ${SERVICES_TENANT_NAME}
# keystone service-create --name heat --type orchestration
# keystone service-create --name heat-cfn --type cloudformation
# keystone endpoint-create --region RegionOne --service-id ${HEAT_CFN_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --adminurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --internalurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot;
# keystone endpoint-create --region RegionOne --service-id ${HEAT_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot; --adminurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s --internalurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Update the paste files at &lt;code&gt;/etc/heat/heat-api{,-cfn,-cloudwatch}-paste.ini&lt;/code&gt; with the credentials just created:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
admin_tenant_name = ${SERVICES_TENANT_NAME}
admin_user = heat
admin_password = ${HEAT_USER_PASSWORD}
&lt;/pre&gt;
&lt;p&gt;In there you also need to make sure that the following variables are pointing to your Keystone host (127.0.0.1 should just work if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt; as Keystone is probably installed on the same host):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
service_host = ${KEYSTONE_HOSTNAME}
auth_host = ${KEYSTONE_HOSTNAME}
auth_uri = http://${KEYSTONE_HOSTNAME}:35357/v2.0
keystone_ec2_uri = http://${KEYSTONE_HOSTNAME}:5000/v2.0/ec2tokens
&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; you've to make instead sure that the following variables &lt;strong&gt;do not&lt;/strong&gt; point to 127.0.0.1 even though the services are actually hosted on the same system because URLs will be passed over to the VMs, which don't have them available locally:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
heat_metadata_server_url = http://${HEAT_CFN_HOSTNAME}:8000
heat_waitcondition_server_url = http://${HEAT_CFN_HOSTNAME}:8000/v1/waitcondition
heat_watch_server_url = http://${HEAT_CLOUDWATCH_HOSTNAME}:8003
&lt;/pre&gt;
&lt;p&gt;The application templates can use wait conditions and signaling for the orchestration, &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; needs to create special users to receive the progress data and these users are, by default, given the role of &lt;code&gt;heat_stack_user&lt;/code&gt;. You can configure the role name in &lt;code&gt;heat-engine.conf&lt;/code&gt; or just create a so called role:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone role-create --name heat_stack_user
&lt;/pre&gt;
&lt;p&gt;The configuration should now be complete and the services can be started:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cd /etc/init.d &amp;amp;&amp;amp; for s in $(ls openstack-heat-*); do chkconfig $s on &amp;amp;&amp;amp; service $s start; done
&lt;/pre&gt;
&lt;p&gt;Make sure by checking the logs that everything was started successfully. Specifically, in case the engine service reports &lt;code&gt;ImportError: cannot import name Random&lt;/code&gt; then you're probably using an old version of &lt;code&gt;pycrypto&lt;/code&gt;. A fix has been merged upstream to workaround the issue. It's &lt;a class="reference external" href="https://review.openstack.org/#/c/26759/"&gt;a trivial change&lt;/a&gt; which you can apply manually to &lt;code&gt;heat/common/crypt.py&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="get-the-demo-files"&gt;
&lt;h2&gt;Get the demo files&lt;/h2&gt;
&lt;p&gt;It is time now to launch your first multi-instance cloud application! There are a number of sample templates available in the &lt;a class="reference external" href="https://github.com/openstack/heat"&gt;github repo&lt;/a&gt;, download the composed Wordpress example with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# wget https://raw.github.com/openstack/heat-templates/master/cfn/WordPress_Composed_Instances.template
&lt;/pre&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; can use the templates distributed for &lt;a class="reference external" href="http://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt;. These expect you to have a well known set of flavor types defined while the default flavors available in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; don't match strictly such a collection. To avoid the need of hack the templates, you can use an helpful script which recreates in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; the same flavors from AWS:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# curl https://raw.github.com/openstack/heat/master/tools/nova_create_flavors.sh | bash
&lt;/pre&gt;
&lt;p&gt;Every template also provides you with a list of usable distros and map these into an AMI string, for each arch. You will have to populate Glance with an image matching the AMI string that the template file is expecting to find.&lt;/p&gt;
&lt;p&gt;There is a tool, called &lt;a class="reference external" href="https://github.com/sdake/heat-jeos"&gt;heat-jeos&lt;/a&gt;, which can be used to create the JEOS images and upload them to Glance but there is also a collection of prebuilt images at: &lt;a class="reference external" href="http://fedorapeople.org/groups/heat/prebuilt-jeos-images/"&gt;http://fedorapeople.org/groups/heat/prebuilt-jeos-images/&lt;/a&gt; so I suggest you to just download one from &lt;code&gt;F17-x86_64-cfntools.qcow2&lt;/code&gt; or &lt;code&gt;U10-x86_64-cfntools.qcow2&lt;/code&gt; (which are referred by many if not all the templates available in the Heat's repo). To upload the F17 x86_64 image in Glance:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# glance image-create --name F17-x86_64-cfntools --disk-format qcow2 --container-format bare --is-public True --copy-from http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2
&lt;/pre&gt;
&lt;p&gt;While that is downloading, create a new keypair or upload you public key in nova to make sure you'll be able to login on the VMs using SSH:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# nova keypair-add --pub_key ~/.ssh/id_rsa.pub userkey
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="launch"&gt;
&lt;h2&gt;Launch!&lt;/h2&gt;
&lt;p&gt;It is time for the real fun now, launch your first composed application with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn create wordpress --template-file=WordPress_Composed_Instances.template --parameters=&amp;quot;DBUsername=wp;DBPassword=wp;KeyName=userkey;LinuxDistribution=F17&amp;quot;
&lt;/pre&gt;
&lt;p&gt;More parameters could have passed, note for instance the LinuxDistribution parameter discussed above. Now the interesting stuff:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn list
# heat-cfn event-list wordpress
&lt;/pre&gt;
&lt;p&gt;After the VMs are launched, the mysql/httpd/wordpress installation and configuration begins, the process is driven by the &lt;code&gt;cfntools&lt;/code&gt;, installed in the VMs images. It will take quite some time, despite the &lt;code&gt;event-list&lt;/code&gt; reporting completion for the WordPress install too early (there is signaling, via &lt;code&gt;cfn-signal&lt;/code&gt;, only in the MySQL template). You can login on the instances and check the logs or just use &lt;code&gt;ps&lt;/code&gt; to see how things are going. After some minutes the setup should be finished:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn describe wordpress
# wget ${WebsiteURL} // that is an URL from the previous command!
&lt;/pre&gt;
&lt;p&gt;If anything goes wrong, check the logs at &lt;code&gt;/var/log/heat/engine.log&lt;/code&gt; or look at the scripts passed as &lt;code&gt;UserData&lt;/code&gt; to the instances, these should be found in &lt;code&gt;/var/lib/cloud/data/&lt;/code&gt;. Time to hack your very own template and delete the test deployment! :)&lt;/p&gt;
&lt;/div&gt;
</summary><category term="openstack"></category><category term="heat"></category><category term="fedoraplanet"></category><category term="rhel"></category><category term="centos"></category></entry></feed>