<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Giulio Fidente - openstack</title><link href="http://giuliofidente.com/" rel="alternate"></link><link href="http://giuliofidente.com/feeds/tag/openstack.atom.xml" rel="self"></link><id>http://giuliofidente.com/</id><updated>2018-03-19T03:32:00+01:00</updated><entry><title>Ceph integration topics at OpenStack PTG</title><link href="http://giuliofidente.com/2018/03/ceph-integration-topics-at-openstack-ptg.html" rel="alternate"></link><published>2018-03-19T03:32:00+01:00</published><updated>2018-03-19T03:32:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2018-03-19:/2018/03/ceph-integration-topics-at-openstack-ptg.html</id><summary type="html">&lt;p&gt;I wanted to share a short summary of the discussions happened around the Ceph integration (in TripleO) at the &lt;a class="reference external" href="https://www.openstack.org/ptg/"&gt;OpenStack PTG&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="ceph-container-ansible-branching"&gt;
&lt;h2&gt;ceph-{container,ansible} branching&lt;/h2&gt;
&lt;p&gt;Together with &lt;a class="reference external" href="http://blog.johnlikesopenstack.com/"&gt;John Fulton&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/guits"&gt;Guillaume Abrioux&lt;/a&gt; (and after PTG, &lt;a class="reference external" href="http://www.sebastien-han.fr/"&gt;Sebastien Han&lt;/a&gt;) we put some thought into how to make the Ceph container images and ceph-ansible releases fit better the OpenStack model; the container images and ceph-ansible are in fact loosely coupled (not all versions of the container images work …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I wanted to share a short summary of the discussions happened around the Ceph integration (in TripleO) at the &lt;a class="reference external" href="https://www.openstack.org/ptg/"&gt;OpenStack PTG&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="ceph-container-ansible-branching"&gt;
&lt;h2&gt;ceph-{container,ansible} branching&lt;/h2&gt;
&lt;p&gt;Together with &lt;a class="reference external" href="http://blog.johnlikesopenstack.com/"&gt;John Fulton&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/guits"&gt;Guillaume Abrioux&lt;/a&gt; (and after PTG, &lt;a class="reference external" href="http://www.sebastien-han.fr/"&gt;Sebastien Han&lt;/a&gt;) we put some thought into how to make the Ceph container images and ceph-ansible releases fit better the OpenStack model; the container images and ceph-ansible are in fact loosely coupled (not all versions of the container images work with all versions of ceph-ansible) and we wanted to move from a &amp;quot;rolling release&amp;quot; into a &amp;quot;point release&amp;quot; approach, mainly to permit regular maintenance of the previous versions known to work with the previous OpenStack versions. The plan goes more or less as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;ceph-{container,ansible} should be released together with the regular ceph updates&lt;/li&gt;
&lt;li&gt;ceph-container will start using tags and stable branches like ceph-ansible does&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The changes for the ceph/daemon docker images are visible already:
&lt;a class="reference external" href="https://hub.docker.com/r/ceph/daemon/tags/"&gt;https://hub.docker.com/r/ceph/daemon/tags/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="multiple-ceph-clusters"&gt;
&lt;h2&gt;Multiple Ceph clusters&lt;/h2&gt;
&lt;p&gt;In the attempt to support better the &amp;quot;edge computing&amp;quot; use case, we discussed adding support for the deployment of multiple Ceph clusters in the overcloud.&lt;/p&gt;
&lt;p&gt;Together with &lt;a class="reference external" href="http://blog.johnlikesopenstack.com/"&gt;John Fulton&lt;/a&gt; and &lt;a class="reference external" href="http://hardysteven.blogspot.com/"&gt;Steven Hardy&lt;/a&gt; (and after PTG, Gregory Charot) we realized this could be done using multiple stacks and by doing so, hopefully simplify managament of the &amp;quot;cells&amp;quot; and avoid potential issues due to orchestration of large clusters.&lt;/p&gt;
&lt;p&gt;Much of this will build on Shardy's blueprint to split the control plane, see spec at: &lt;a class="reference external" href="https://review.openstack.org/#/c/523459/"&gt;https://review.openstack.org/#/c/523459/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The multiple Ceph clusters specifics will be tracked via another blueprint: &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/deploy-multiple-ceph-clusters"&gt;https://blueprints.launchpad.net/tripleo/+spec/deploy-multiple-ceph-clusters&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ceph-ansible-testing-with-tripleo"&gt;
&lt;h2&gt;ceph-ansible testing with TripleO&lt;/h2&gt;
&lt;p&gt;We had a very good chat with &lt;a class="reference external" href="http://blog.johnlikesopenstack.com/"&gt;John Fulton&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/guits"&gt;Guillaume Abrioux&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/weshayutin"&gt;Wesley Hayutin&lt;/a&gt; and &lt;a class="reference external" href="https://jpenatech.wordpress.com/"&gt;Javier Pena&lt;/a&gt; on how to get tested new pull requests for ceph-ansible with TripleO; basically trigger an existing TripleO scenario on changes proposed to ceph-ansible.&lt;/p&gt;
&lt;p&gt;Given ceph-ansible is hosted on github, Wesley's and Javier suggested this should be possible with Zuul v3 and volunteered to help; some of the complications are about building an RPM from uncommitted changes for testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="move-ceph-ansible-triggering-from-workflow-tasks-to-external-deploy-tasks"&gt;
&lt;h2&gt;Move ceph-ansible triggering from workflow_tasks to external_deploy_tasks&lt;/h2&gt;
&lt;p&gt;This is a requirement for the Rocky release; we want to migrate away from using workflow_tasks and use external_deploy_tasks instead, to integrate into the &amp;quot;config-download&amp;quot; mechanism.&lt;/p&gt;
&lt;p&gt;This work is tracked via a blueprint and we have a WIP submission on review: &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/ceph-ansible-external-deploy-tasks"&gt;https://blueprints.launchpad.net/tripleo/+spec/ceph-ansible-external-deploy-tasks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We're also working with Sofer Athlan-Guyot on the enablement of Ceph in the upgrade CI jobs and with Tom Barron on scenario004 to deploy Manila with Ganesha (and CephFS) instead of the CephFS native backend.&lt;/p&gt;
&lt;p&gt;Hopefully I didn't forget much; to stay updated on the progress join &lt;code&gt;#tripleo&lt;/code&gt; on freenode or check our integration squad status at: &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-integration-squad-status"&gt;https://etherpad.openstack.org/p/tripleo-integration-squad-status&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</content><category term="openstack"></category><category term="ceph"></category><category term="ceph-ansible"></category><category term="tripleo"></category></entry><entry><title>Understanding ceph-ansible in TripleO</title><link href="http://giuliofidente.com/2017/07/understanding-ceph-ansible-in-tripleo.html" rel="alternate"></link><published>2017-07-19T11:00:00+02:00</published><updated>2017-07-19T11:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2017-07-19:/2017/07/understanding-ceph-ansible-in-tripleo.html</id><summary type="html">&lt;p&gt;One of the goals for the TripleO Pike release was to introduce ceph-ansible as an alternative to puppet-ceph for the deployment of Ceph.&lt;/p&gt;
&lt;p&gt;More specifically, to put operators in control of the playbook execution as if they were launching ceph-ansible from the commandline, except it would be Heat starting ceph-ansible at the right time during the overcloud deployment.&lt;/p&gt;
&lt;p&gt;This demanded for &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-ceph-ansible"&gt;some changes&lt;/a&gt; in different tools used by TripleO and went through a pretty long …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the goals for the TripleO Pike release was to introduce ceph-ansible as an alternative to puppet-ceph for the deployment of Ceph.&lt;/p&gt;
&lt;p&gt;More specifically, to put operators in control of the playbook execution as if they were launching ceph-ansible from the commandline, except it would be Heat starting ceph-ansible at the right time during the overcloud deployment.&lt;/p&gt;
&lt;p&gt;This demanded for &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-ceph-ansible"&gt;some changes&lt;/a&gt; in different tools used by TripleO and went through a pretty long review process, eventually putting in place some useful bits for the future &lt;a class="reference external" href="https://review.openstack.org/471759"&gt;integration of Kubernetes&lt;/a&gt; and migration to an &lt;a class="reference external" href="https://review.openstack.org/483929"&gt;ansible driven deployment&lt;/a&gt; of the overcloud configuration steps in TripleO.&lt;/p&gt;
&lt;p&gt;The idea was to add a generic functionality allowing triggering of a given Mistral workflow during the deployment of a service. Mistral could have then executed any action, including for example an ansible playbook, provided it was given all the necessay input data for the playbook to run and the roles list to build the hosts inventory.&lt;/p&gt;
&lt;p&gt;This is how we did it.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Run ansible-playbook from Mistral (1)&lt;/dt&gt;
&lt;dd&gt;An initial submission added support for the execution of ansible playbooks as workflow tasks in Mistral &lt;a class="reference external" href="https://github.com/openstack/tripleo-common/commit/e6c8a46f00436edfa5de92e97c3a390d90c3ce54"&gt;https://github.com/openstack/tripleo-common/commit/e6c8a46f00436edfa5de92e97c3a390d90c3ce54&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;A generic action for Mistral which workflows can use to run an ansible playbook. +2 to &lt;a class="reference external" href="http://www.dougalmatthews.com/"&gt;Dougal&lt;/a&gt; and &lt;a class="reference external" href="https://twitter.com/rjbrady"&gt;Ryan&lt;/a&gt;.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Deploy external resources from Heat (2)&lt;/dt&gt;
&lt;dd&gt;We also needed a new resource in Heat to be able to drive Mistral workflow executions &lt;a class="reference external" href="https://github.com/openstack/heat/commit/725b404468bdd2c1bdbaf16e594515475da7bace"&gt;https://github.com/openstack/heat/commit/725b404468bdd2c1bdbaf16e594515475da7bace&lt;/a&gt; so that we could orchestrate the executions like any other Heat resource. This is described much in detail in a &lt;a class="reference external" href="https://specs.openstack.org/openstack/heat-specs/specs/liberty/external_resource.html"&gt;Heat spec&lt;/a&gt;.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;With these two, we could run an ansible playbook from a Heat resource, via Mistral. +2 to &lt;a class="reference external" href="http://www.zerobanana.com/"&gt;Zane&lt;/a&gt; and &lt;a class="reference external" href="https://twitter.com/therve"&gt;Thomas&lt;/a&gt; for the help! Enough to start messing in TripleO and glue things together.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Describe what/when to run in TripleO (3)&lt;/dt&gt;
&lt;dd&gt;We added a mechanim in the TripleO templates to make it possible to describe, from within a service, a list of tasks or workflows to be executed at any given deployment step &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates/commit/71f13388161cbab12fe284f7b251ca8d36f7635c"&gt;https://github.com/openstack/tripleo-heat-templates/commit/71f13388161cbab12fe284f7b251ca8d36f7635c&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;There aren't restrictions on what the tasks or workflows in the new section should do. These might deploy the service or prepare the environment for it or execute code (eg. build Swift rings). The commit message explains how to use it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
service_workflow_tasks:
  step2:
    - name: my_action_name
      action: std.echo
      input:
        output: 'hello world'
&lt;/pre&gt;
&lt;p&gt;The above snippet would make TripleO to run the Mistral &lt;code&gt;std.echo&lt;/code&gt; action during the overcloud deployment, precisely at step 2, assuming you create a new service with the code above and enable it on a role.&lt;/p&gt;
&lt;p&gt;For Ceph we wanted to run the new Mistral action (1) and needed to provide it with the config settings for the service, normally described within the &lt;code&gt;config_settings&lt;/code&gt; structure of the service template.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Provide config_settings to the workflows (4)&lt;/dt&gt;
&lt;dd&gt;The decision was to make available all config settings into the Mistral execution environment so that ansible actions could, for example, use them as &lt;code&gt;extra_vars&lt;/code&gt; &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates/commit/8b81b363fd48b0080b963fd2b1ab6bfe97b0c204"&gt;https://github.com/openstack/tripleo-heat-templates/commit/8b81b363fd48b0080b963fd2b1ab6bfe97b0c204&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now all config settings normally consumed by puppet were available to the Mistral action and playbook settings could be added too, +2 &lt;a class="reference external" href="http://hardysteven.blogspot.it/"&gt;Steven&lt;/a&gt;.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Build the data for the hosts inventory (5)&lt;/dt&gt;
&lt;dd&gt;Together with the above, another small change provided into the execution environment a dictionary mapping every enabled service to the list of IP address of the nodes where the service is deployed &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates/commit/9c1940e461867f2ce986a81fa313d7995592f0c5"&gt;https://github.com/openstack/tripleo-heat-templates/commit/9c1940e461867f2ce986a81fa313d7995592f0c5&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;This was necessary to be able to build the ansible hosts inventory.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Create a workflow for ceph-ansible (6)&lt;/dt&gt;
&lt;dd&gt;Having all pieces available to trigger the workflow and pass to it the service config settings, we needed the workflow which would run ceph-ansible &lt;a class="reference external" href="https://github.com/openstack/tripleo-common/commit/fa0b9f52080580b7408dc6f5f2da6fc1dc07d500"&gt;https://github.com/openstack/tripleo-common/commit/fa0b9f52080580b7408dc6f5f2da6fc1dc07d500&lt;/a&gt; plus some new, generic Mistral actions, to run smoothly multiple times (eg. stack updates) &lt;a class="reference external" href="https://github.com/openstack/tripleo-common/commit/f81372d85a0a92de455eeaa93162faf09be670cf"&gt;https://github.com/openstack/tripleo-common/commit/f81372d85a0a92de455eeaa93162faf09be670cf&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;This is the glue which runs a ceph-ansible playbook with the given set of parameters. +2 &lt;a class="reference external" href="http://johnlikesopenstack.com"&gt;John&lt;/a&gt;.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;Deploy Ceph via ceph-ansible (7)&lt;/dt&gt;
&lt;dd&gt;Finally, the new services definition for Tripleo &lt;a class="reference external" href="https://review.openstack.org/#/c/465066/"&gt;https://review.openstack.org/#/c/465066/&lt;/a&gt; to deploy Ceph in containers via ceph-ansible, including a couple of params operators can use to push into the Mistral environment arbitrary &lt;code&gt;extra_vars&lt;/code&gt; for ceph-ansible.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;The deployment with ceph-ansible is activated with the ceph-ansible.yaml environment file.&lt;/p&gt;
&lt;p&gt;Interestingly the templates to deploy Ceph using puppet-ceph are unchanged and continue to work as they used to so that for new deployments it is possible to use alternatively the new implementation with ceph-ansible or the pre-existing implementation using puppet-ceph. Only ceph-ansible allows for the deployment of Ceph in containers.&lt;/p&gt;
&lt;p&gt;Big +2 also to &lt;a class="reference external" href="https://github.com/jistr"&gt;Jiri&lt;/a&gt; (who doesn't even need a blog or twitter) and all the people who helped during the development process with feedback, commits and reviews.&lt;/p&gt;
&lt;p&gt;Soon another article with some usage examples and debugging instructions!&lt;/p&gt;
</content><category term="openstack"></category><category term="ceph"></category><category term="tripleo"></category><category term="heat"></category><category term="mistral"></category><category term="ceph-ansible"></category></entry><entry><title>TripleO to deploy Ceph standlone</title><link href="http://giuliofidente.com/2016/12/tripleo-to-deploy-ceph-standlone.html" rel="alternate"></link><published>2016-12-16T23:00:00+01:00</published><updated>2016-12-16T23:00:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2016-12-16:/2016/12/tripleo-to-deploy-ceph-standlone.html</id><summary type="html">&lt;p&gt;Here is a nice Christmas present: you can use TripleO for a standalone Ceph deployment, with just a few lines of YAML. Assuming you have an undercloud ready for a new overcloud, create an environment file like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
resource_registry:
  OS::TripleO::Services::CephMon: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-mon.yaml
  OS::TripleO::Services::CephOSD: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-osd.yaml

parameters:
  ControllerServices:
    - OS::TripleO::Services::CephMon
  CephStorageServices:
    - OS::TripleO::Services::CephOSD
&lt;/pre&gt;
&lt;p&gt;and launch …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here is a nice Christmas present: you can use TripleO for a standalone Ceph deployment, with just a few lines of YAML. Assuming you have an undercloud ready for a new overcloud, create an environment file like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
resource_registry:
  OS::TripleO::Services::CephMon: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-mon.yaml
  OS::TripleO::Services::CephOSD: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-osd.yaml

parameters:
  ControllerServices:
    - OS::TripleO::Services::CephMon
  CephStorageServices:
    - OS::TripleO::Services::CephOSD
&lt;/pre&gt;
&lt;p&gt;and launch a deployment with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
openstack overcloud deploy --compute-scale 0 --ceph-storage-scale 1 -e the_above_env_file.yaml
&lt;/pre&gt;
&lt;p&gt;The two lines from the environment file in &lt;code&gt;resource_registry&lt;/code&gt; are mapping (and enabling) the CephMon and CephOSD services in TripleO while the lines in &lt;code&gt;parameters&lt;/code&gt; are defining which services should be deployed on the &lt;em&gt;controller&lt;/em&gt; and &lt;em&gt;cephstorage&lt;/em&gt; roles.&lt;/p&gt;
&lt;p&gt;This will bring up a two nodes overcloud with one node running ceph-mon and the other ceph-osd but the actual Christmas gift is that it implicitly provides and allows usage of all the features we already know about TripleO, like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;baremetal provisioning&lt;/li&gt;
&lt;li&gt;network isolation&lt;/li&gt;
&lt;li&gt;a web GUI&lt;/li&gt;
&lt;li&gt;lifecycle management&lt;/li&gt;
&lt;li&gt;... &lt;a class="reference external" href="https://review.openstack.org/#/c/223182/"&gt;containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;... &lt;a class="reference external" href="https://review.openstack.org/#/c/392116/"&gt;upgrades&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, you can scale up the Ceph cluster with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
openstack overcloud deploy --compute-scale 0 --ceph-storage-scale 2 -e the_above_env_file.yaml
&lt;/pre&gt;
&lt;p&gt;and this will provision a new Ironic node with the &lt;em&gt;cephstorage&lt;/em&gt; role, configuring the required networks on it and updating the cluster config for the new OSDs. (Note the &lt;code&gt;--ceph-storage-scale&lt;/code&gt; parameter going from 1 to 2 in the second example).&lt;/p&gt;
&lt;p&gt;Even more interestingly is that the above will work for any service, not just Ceph, and new services can be added to TripleO with just some YAML and puppet, letting TripleO take care of a number of common issues in any deployment tool, for example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;supports multinode deployments&lt;/li&gt;
&lt;li&gt;synchronizes and orders the deployment steps across different nodes&lt;/li&gt;
&lt;li&gt;supports propagation of config data across different services&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Time to try it and join the fun in #tripleo :)&lt;/p&gt;
</content><category term="openstack"></category><category term="ceph"></category><category term="tripleo"></category></entry><entry><title>Ceph, TripleO and the Newton release</title><link href="http://giuliofidente.com/2016/08/ceph-tripleo-and-the-newton-release.html" rel="alternate"></link><published>2016-08-26T05:00:00+02:00</published><updated>2016-08-26T05:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2016-08-26:/2016/08/ceph-tripleo-and-the-newton-release.html</id><summary type="html">&lt;p&gt;Time to roll up some notes on the status of Ceph in TripleO. The majority of these functionalities were available in the Mitaka release too but the examples work with code from the Newton release so they might not apply identical to Mitaka.&lt;/p&gt;
&lt;div class="section" id="the-tripleo-default-configuration"&gt;
&lt;h2&gt;The TripleO default configuration&lt;/h2&gt;
&lt;p&gt;No default is going to fit everybody, but we want to know what the default is to improve from there. So let's try and see:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ openstack overcloud …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Time to roll up some notes on the status of Ceph in TripleO. The majority of these functionalities were available in the Mitaka release too but the examples work with code from the Newton release so they might not apply identical to Mitaka.&lt;/p&gt;
&lt;div class="section" id="the-tripleo-default-configuration"&gt;
&lt;h2&gt;The TripleO default configuration&lt;/h2&gt;
&lt;p&gt;No default is going to fit everybody, but we want to know what the default is to improve from there. So let's try and see:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ openstack overcloud deploy --templates tripleo-heat-templates -e tripleo-heat-templates/environments/puppet-pacemaker.yaml -e tripleo-heat-templates/environments/storage-environment.yaml --ceph-storage-scale 1
Deploying templates in the directory /home/stack/example/tripleo-heat-templates
...
Overcloud Deployed
&lt;/pre&gt;
&lt;p&gt;Monitors go on the &lt;cite&gt;controller&lt;/cite&gt; nodes, one per node, the above command is deploying a single controller though. First interesting thing to point out is:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph --version
ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)
&lt;/pre&gt;
&lt;p&gt;Jewel! Kudos to &lt;a class="reference external" href="http://my1.fr/blog/"&gt;Emilien&lt;/a&gt; for bringing support for it in &lt;code&gt;puppet-ceph&lt;/code&gt;. Continuing our investigation, we notice the OSDs go on the &lt;cite&gt;cephstorage&lt;/cite&gt; nodes and are backed by the local filesystem, as we didn't tell it to do differently:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph osd tree
ID WEIGHT  TYPE NAME                        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.03999 root default
-2 0.03999     host overcloud-cephstorage-0
 0 0.03999         osd.0                         up  1.00000          1.00000
&lt;/pre&gt;
&lt;p&gt;Notice we got SELinux covered:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ls -laZ /srv/data
drwxr-xr-x. ceph ceph system_u:object_r:ceph_var_lib_t:s0 .
...
&lt;/pre&gt;
&lt;p&gt;And use CephX with autogenerated keys:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph auth list
installed auth entries:

client.admin
        key: AQC2Pr9XAAAAABAAOpviw6DqOMG0syeEYmX2EQ==
        caps: [mds] allow *
        caps: [mon] allow *
        caps: [osd] allow *
client.openstack
        key: AQC2Pr9XAAAAABAAA78Svmmt+LVIcRrZRQLacw==
        caps: [mon] allow r
        caps: [osd] allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=backups, allow rwx pool=vms, allow rwx pool=images, allow rwx pool=metrics
&lt;/pre&gt;
&lt;p&gt;But which OpenStack service is using Ceph? The &lt;code&gt;storage-environment.yaml&lt;/code&gt; file has some informations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ grep -v '#' tripleo-heat-templates/environments/storage-environment.yaml | uniq

 resource_registry:
   OS::TripleO::Services::CephMon: ../puppet/services/ceph-mon.yaml
   OS::TripleO::Services::CephOSD: ../puppet/services/ceph-osd.yaml
   OS::TripleO::Services::CephClient: ../puppet/services/ceph-client.yaml

 parameter_defaults:
   CinderEnableIscsiBackend: false
   CinderEnableRbdBackend: true
   CinderBackupBackend: ceph
   NovaEnableRbdBackend: true
   GlanceBackend: rbd
   GnocchiBackend: rbd
&lt;/pre&gt;
&lt;p&gt;The registry lines enable the Ceph services, the parameters instead are setting Ceph as backend for Cinder, Nova, Glance and Gnocchi. They can be configured to use other backends, see the comments in the environment file. Regarding the pools:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph osd lspools
0 rbd,1 metrics,2 images,3 backups,4 volumes,5 vms,
&lt;/pre&gt;
&lt;p&gt;Despite the replica size set by default to 3, we only have a single OSD so with a single OSD the cluster will never get into HEALTH_OK:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph osd pool get vms size
size: 3
&lt;/pre&gt;
&lt;p&gt;Good to know, now a new deployment with more interesting stuff.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-more-realistic-scenario"&gt;
&lt;h2&gt;A more realistic scenario&lt;/h2&gt;
&lt;p&gt;What makes it &amp;quot;more realistic&amp;quot;? We'll have enough OSDs to cover the replica size. We'll use physical disks for our OSDs (and journals) and not the local filesystem. We'll cope with a node with a different disks topology and we'll decrease the replica size for one of the pools.&lt;/p&gt;
&lt;div class="section" id="set-a-default-disks-map-for-the-osd-nodes"&gt;
&lt;h3&gt;Set a default disks map for the OSD nodes&lt;/h3&gt;
&lt;p&gt;Define a default configuration for the storage nodes, telling TripleO to use &lt;code&gt;sdb&lt;/code&gt; for the OSD data and &lt;code&gt;sdc&lt;/code&gt; for the journal:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ceph_default_disks.yaml
  parameter_defaults:
    CephStorageExtraConfig:
      ceph::profile::params::osds:
        /dev/sdb:
          journal: /dev/sdc
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="customize-the-disks-map-for-a-specific-node"&gt;
&lt;h3&gt;Customize the disks map for a specific node&lt;/h3&gt;
&lt;p&gt;For the node which has two (instead of a single) rotatory disks, we'll need a specific map. First get its system-uuid from the Ironic introspection data:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ openstack baremetal introspection data save | jq .extra.system.product.uuid
&amp;quot;66C033FA-BAC0-4364-9E8A-3184B5952370&amp;quot;
&lt;/pre&gt;
&lt;p&gt;then create the node specific map:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ceph_mynode_disks.yaml
  resource_registry:
    OS::TripleO::CephStorageExtraConfigPre: tripleo-heat-templates/puppet/extraconfig/pre_deploy/per_node.yaml

  parameter_defaults:
    NodeDataLookup: &amp;gt;
     {&amp;quot;66C033FA-BAC0-4364-9E8A-3184B5952370&amp;quot;:
       {&amp;quot;ceph::profile::params::osds&amp;quot;:
         {&amp;quot;/dev/sdb&amp;quot;: {&amp;quot;journal&amp;quot;: &amp;quot;/dev/sdd&amp;quot;},
          &amp;quot;/dev/sdc&amp;quot;: {&amp;quot;journal&amp;quot;: &amp;quot;/dev/sdd&amp;quot;}
         }
       }
     }
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="fine-tune-pg-num-pgp-num-and-replica-size-for-a-pool"&gt;
&lt;h3&gt;Fine tune pg_num, pgp_num and replica size for a pool&lt;/h3&gt;
&lt;p&gt;Finally, to override the replica size (and why not, PGs number) of the &amp;quot;vms&amp;quot; pool (where by default the Nova ephemeral disks go):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ceph_pools_config.yaml
  parameter_defaults:
    CephPools:
      vms:
        size: 2
        pg_num: 128
        pgp_num: 128
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="zap-all-disks-for-the-new-deployment"&gt;
&lt;h3&gt;Zap all disks for the new deployment&lt;/h3&gt;
&lt;p&gt;We also want to clear and prepare all the non-root disks with a GPT label, which will allow us, for example, to repeat the deployment multiple times reusing the same nodes. The implementation of the disks cleanup script can vary, but we can use &lt;a class="reference external" href="https://gist.github.com/gfidente/42d3cdfe0c67f7c95f0c"&gt;a sample script&lt;/a&gt; and wire it to the overcloud nodes via &lt;cite&gt;NodeUserData&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ curl -O https://gist.githubusercontent.com/gfidente/42d3cdfe0c67f7c95f0c/raw/1f467c6018ada194b54f22113522db61ef944e20/ceph_wipe_disk.yaml

ceph_wipe_env.yaml:
  resource_registry:
    OS::TripleO::NodeUserData: ceph_wipe_disk.yaml

  parameter_defaults:
    ceph_disks: &amp;quot;/dev/sdb /dev/sdc /dev/sdd&amp;quot;
&lt;/pre&gt;
&lt;p&gt;All the above environment files could have been merged in a single one but we split them out in multiple ones for clarity. Now the new deploy command:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
uc$ openstack overcloud deploy --templates tripleo-heat-templates -e tripleo-heat-templates/environments/puppet-pacemaker.yaml -e tripleo-heat-templates/environments/storage-environment.yaml --ceph-storage-scale 3 -e ceph_pools_config.yaml -e ceph_mynode_disks.yaml -e ceph_default_disks.yaml -e ceph_wipe_env.yaml
Deploying templates in the directory /home/stack/example/tripleo-heat-templates
...
Overcloud Deployed
&lt;/pre&gt;
&lt;p&gt;Here is our OSDs tree, with two instances running on the node with two rotatory disks (sharing the same journal disk):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph os tree
ID WEIGHT  TYPE NAME                        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.03119 root default
-2 0.00780     host overcloud-cephstorage-1
 0 0.00780         osd.0                         up  1.00000          1.00000
-3 0.01559     host overcloud-cephstorage-2
 1 0.00780         osd.1                         up  1.00000          1.00000
 2 0.00780         osd.2                         up  1.00000          1.00000
-4 0.00780     host overcloud-cephstorage-0
 3 0.00780         osd.3                         up  1.00000          1.00000
&lt;/pre&gt;
&lt;p&gt;and the custom PG/size values for for &amp;quot;vms&amp;quot; pool:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
oc$ ceph osd pool get vms size
size: 2
oc$ ceph osd pool get vms pg_num
pg_num: 128
&lt;/pre&gt;
&lt;p&gt;Another simple customization could have been to set the journals size. For example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ceph_journal_size.yaml
  parameter_defaults:
    ExtraConfig:
      ceph::profile::params::osd_journal_size: 1024
&lt;/pre&gt;
&lt;p&gt;Also we did not provide any customization for the &lt;cite&gt;crushmap&lt;/cite&gt; but &lt;a class="reference external" href="https://github.com/openstack/puppet-ceph/commit/b1af406398488df7fc3d35263451f3e2ad802b9b"&gt;a recent addition&lt;/a&gt; from &lt;a class="reference external" href="https://twitter.com/epkuva"&gt;Erno&lt;/a&gt; makes it possible to disable &lt;code&gt;global/osd_crush_update_on_start&lt;/code&gt; so that any customization becomes possible after the deployment is finished.&lt;/p&gt;
&lt;p&gt;Also we did not deploy the RadosGW service as it is still a work in progress, expected for the Newton release. Submissions for its inclusion are &lt;a class="reference external" href="https://review.openstack.org/#/c/289027/"&gt;on review&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We're also working on automating the upgrade from the Ceph/Hammer release deployed with TripleO/Mitaka to Ceph/Jewel, installed with TripleO/Newton. The process will be integrated with the OpenStack upgrade and again the submissions are &lt;a class="reference external" href="https://review.openstack.org/#/c/359410/"&gt;on review in a series&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="for-more-scenarios"&gt;
&lt;h2&gt;For more scenarios&lt;/h2&gt;
&lt;p&gt;The mechanism recently introduced in TripleO to make composable roles, discussed in a &lt;a class="reference external" href="http://hardysteven.blogspot.it/2016/08/tripleo-composable-services-101.html"&gt;Steven&lt;/a&gt;'s blog post, makes it possible to test a complete Ceph deployment using a single controller node too (hosting the OSD service as well), just by adding &lt;code&gt;OS::TripleO::Services::CephOSD&lt;/code&gt; to the list of services deployed on the controller role.&lt;/p&gt;
&lt;p&gt;And if the above still wasn't enough, TripleO continues to support configuration of OpenStack with a pre-existing, unmanaged Ceph cluster. To do so we'll want to customize the parameters in &lt;code&gt;puppet-ceph-external.yaml&lt;/code&gt; and deploy passing that as argument instead. For example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
puppet-ceph-external.yaml
  resource_registry:
    OS::TripleO::Services::CephExternal: tripleo-heat-templates/puppet/services/ceph-external.yaml

  parameter_defaults:
    # NOTE: These example parameters are required when using Ceph External and must be obtained from the running cluster
    #CephClusterFSID: '4b5c8c0a-ff60-454b-a1b4-9747aa737d19'
    #CephClientKey: 'AQDLOh1VgEp6FRAAFzT7Zw+Y9V6JJExQAsRnRQ=='
    #CephExternalMonHost: '172.16.1.7, 172.16.1.8'

    # the following parameters enable Ceph backends for Cinder, Glance, Gnocchi and Nova
    NovaEnableRbdBackend: true
    CinderEnableRbdBackend: true
    CinderBackupBackend: ceph
    GlanceBackend: rbd
    GnocchiBackend: rbd
    # If the Ceph pools which host VMs, Volumes and Images do not match these
    # names OR the client keyring to use is not named 'openstack',  edit the
    # following as needed.
    NovaRbdPoolName: vms
    CinderRbdPoolName: volumes
    GlanceRbdPoolName: images
    GnocchiRbdPoolName: metrics
    CephClientUserName: openstack
    # finally we disable the Cinder LVM backend
    CinderEnableIscsiBackend: false
&lt;/pre&gt;
&lt;p&gt;Come help in #tripleo &amp;#64; freenode and don't forget to check the docs at &lt;a class="reference external" href="http://tripleo.org"&gt;tripleo.org&lt;/a&gt;! Some related topics are described there, for example, how to &lt;a class="reference external" href="http://tripleo.org/advanced_deployment/root_device.html"&gt;set the root device&lt;/a&gt; via Ironic for the nodes with multiple disks or how to &lt;a class="reference external" href="http://tripleo.org/advanced_deployment/ceph_config.html"&gt;push in ceph.conf&lt;/a&gt; additional arbitraty settings.&lt;/p&gt;
&lt;/div&gt;
</content><category term="openstack"></category><category term="ceph"></category><category term="tripleo"></category></entry><entry><title>OpenStack summit in Tokyo and TripleO</title><link href="http://giuliofidente.com/2015/11/openstack-summit-in-tokyo-and-tripleo.html" rel="alternate"></link><published>2015-11-03T17:30:00+01:00</published><updated>2015-11-03T17:30:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-11-03:/2015/11/openstack-summit-in-tokyo-and-tripleo.html</id><summary type="html">&lt;p&gt;&lt;a class="reference external" href="https://www.openstack.org/summit/tokyo-2015/"&gt;OpenStack summit Tokyo&lt;/a&gt; anyone? I've been there and thought it was a very well organized event, in a nice location. Every minute together with peers seemed worth it to me. This said, let's talk about the actual sessions. I spent most of my time at the TripleO and Heat sessions, with a little detour on Magnum. Plus some booth crawling.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;TripleO&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;We had three sessions and a full day meetup. The first discussed the recent …&lt;/p&gt;&lt;/dd&gt;&lt;/dl&gt;</summary><content type="html">&lt;p&gt;&lt;a class="reference external" href="https://www.openstack.org/summit/tokyo-2015/"&gt;OpenStack summit Tokyo&lt;/a&gt; anyone? I've been there and thought it was a very well organized event, in a nice location. Every minute together with peers seemed worth it to me. This said, let's talk about the actual sessions. I spent most of my time at the TripleO and Heat sessions, with a little detour on Magnum. Plus some booth crawling.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;TripleO&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;We had three sessions and a full day meetup. The first discussed the recent work done to allow for the deployment of OpenStack in containers, using TripleO; the changes to deploy the compute nodes and a majority of the controller roles in containers are up for review, which is great; some notes can be found in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-containers"&gt;tripleo-mitaka-containers&lt;/a&gt; pad. It was very nice for me to see the TripleO bits delivering on the promise of being flexibile; deploying in containers requires minimal changes to the core tools and yet inherits important features like network isolation and support for Ceph.&lt;/p&gt;
&lt;p&gt;Another session was on upgrades, which isn't a trivial goal as it pushes on both Heat and Puppet. The proposed plan is to start with a CI job attempting to upgrade an overcloud from the stable branch to the master branch; more in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-kilo-to-liberty-upgrades"&gt;tripleo-kilo-to-liberty-upgrades&lt;/a&gt; pad. I will hopefully write more about this topic in future posts.&lt;/p&gt;
&lt;p&gt;There has also been a session on the CLI and UI clients. Again, not a simple problem because the Heat templates are constantly ongoing refactor to implement new features and because putting too much business logic into the client limited our flexibility in the past. We'll probably see a shared library for the newer CLI/UI, with less business logic and probably a REST API, more in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-restapi"&gt;tripleo-mitaka-restapi&lt;/a&gt; pad.&lt;/p&gt;
&lt;p class="last"&gt;Then the community meetup on Friday covered a few additional topics. We reviewed the status of the CI with ideas for more jobs, discussed some ideas to provision the puppet modules via Heat instead of shipping them with the images, collected lots of good feedback from actual OpenStack operators and, last but not least, discussed the recent work from &lt;a class="reference external" href="https://github.com/dprince/"&gt;Dan Prince&lt;/a&gt; to have composable roles. Kudos to Dan on the composable roles which also seems to work pretty well with the effort to containerize the deployment. Notes about the community meetup are in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/tripleo-mitaka-meetup"&gt;tripleo-mitaka-meetup&lt;/a&gt; pad.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Heat&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;I've joined only those sessions were I could at least understand the topics and yet there is quite a lot to write about. First an interesting session focused on a problem with dependencies across nested stacks, which we make large us of in TripleO. Both the issue and the consequences are much easier to understand with a picture than with words. &lt;a class="reference external" href="http://www.zerobanana.com/"&gt;Zane Bitter&lt;/a&gt; had indeed prepared some &lt;em&gt;demo&lt;/em&gt; material for the session, including pictures, which should be linked from the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-break-stack-barrier"&gt;mitaka-heat-break-stack-barrier&lt;/a&gt; pad.&lt;/p&gt;
&lt;p&gt;Then a session to discuss issues affecting large stack deployments. Major points for discussion were of three types: token timeouts, batching of operations and resources status polling, with resources polling being probably the hardest to cope with. More in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-large-stacks"&gt;mitaka-heat-large-stacks&lt;/a&gt; pad.&lt;/p&gt;
&lt;p&gt;Then a session to collect input from users and ops. My proposal here was for the introduction of 'immutable resources' where resources with such a proerty wouldn't get deleted or updated during a stack update. One proposed solution for this is a warning message, outputted by the stack validation process, in case of resources deletion. More on this and on the session in general in the &lt;a class="reference external" href="https://etherpad.openstack.org/p/mitaka-heat-user-ops"&gt;mitaka-heat-user-ops&lt;/a&gt; pad.&lt;/p&gt;
&lt;p class="last"&gt;There has also been an interesting conversation about the changes from a spec meant to introduce support for &lt;a class="reference external" href="http://specs.openstack.org/openstack/heat-specs/specs/liberty/external_resource.html"&gt;external resources&lt;/a&gt;, which is something TripleO could use soon.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Magnum&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;The Magnum project seems to be facing some of the problems which TripleO faced in past as well, in relation to the Heat templates. The goal seems to be to customize the templates based on the input parameters; I joined the session to learn more about the Magnum approach and the proposed solutions.&lt;/p&gt;
&lt;p class="last"&gt;It seems to me that while TripleO relies on actual Heat features to allow for different implementations of the same resource, via resource registry, Magnum is working instead on a sort of pre-processor (powered by Jinja) which generates Heat templates using conditionals and includes from the Jinja language. More on this is in the &lt;a class="reference external" href="https://bugs.launchpad.net/magnum/+bug/1501045"&gt;launchpad bug 1501045&lt;/a&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Who said installers?&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p class="first"&gt;It was nice to see TripleO mentioned in at least two more sessions, one where Mirantis directly compared Fuel to the Red Hat OpenStack Platform Director and another where &lt;a class="reference external" href="https://openstacksummitoctober2015tokyo.sched.org/event/7032267fad9f6f0e5242093d17d59d64"&gt;four different automated installers were reviewed&lt;/a&gt;. Each had its moments.&lt;/p&gt;
&lt;p class="last"&gt;I think one good thing about TripleO is that it is pretty flexible and offers some capabilities to maintain the environment even after the initial installation; hopefully both will make it interesting to many.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;RDO meetup&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;We had a meetup for the &lt;a class="reference external" href="http://rdoproject.org/"&gt;RDO&lt;/a&gt; project and it cleared up a lot of questions on how the packages are built and tested for both the trunk and the stable branches. Good news is that it is now possible to try TripleO, on CentOS, with the latest RDO bits. The docs at &lt;a class="reference external" href="http://tripleo.org"&gt;tripleo.org&lt;/a&gt; are documenting exactly that.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now, what about the event? Well, as an ATC, I have two things to say about the event more in general. First, &lt;strong&gt;I'd really like the design summit to start on Tuesday&lt;/strong&gt;. Simply put, there are so many sessions going on that it is impossible to join anything out of the most immediate interests. My thinking is that one more day could at least lower parallelism!&lt;/p&gt;
&lt;p&gt;Another comment is, please &lt;strong&gt;make the keynotes a little less marketish&lt;/strong&gt;. I know the circus runs on actual money but my feeling is that the keynotes, on both Tuesday and Wednesday, left some sessions go a little too far into the product marketing area when they were probably supposed to present an actual OpenStack use case.&lt;/p&gt;
&lt;img alt="OpenStack summit Ticket" class="align-center" src="http://giuliofidente.com/images/ticket_os2015.png" /&gt;
&lt;p&gt;Last but not least, it was great to have some time to meet face to face peers and coworkers from all over the world. Including some who are &lt;em&gt;not exactly&lt;/em&gt; peers, like &lt;a class="reference external" href="https://www.linkedin.com/in/matthicksj"&gt;Matt&lt;/a&gt;, to whom I ended up asking if he was a newcome to the TripleO project during dinner. I'm sure next time I will remember and do better.&lt;/p&gt;
</content><category term="openstack"></category><category term="events"></category><category term="tripleo"></category><category term="fedoraplanet"></category></entry><entry><title>On TripleO and the EnablePacemaker bool</title><link href="http://giuliofidente.com/2015/05/on-tripleo-and-the-enablepacemaker-bool.html" rel="alternate"></link><published>2015-05-01T15:02:00+02:00</published><updated>2015-05-01T15:02:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-05-01:/2015/05/on-tripleo-and-the-enablepacemaker-bool.html</id><summary type="html">&lt;p&gt;Yes, EnablePacemaker! A little old fashioned and not very well suited for OpenStack you say heh? Let's try to talk a bit about this then. News is that we're adding into TripleO/Puppet an option to deploy Pacemaker on the Overcloud nodes. Puppet you said? Sure TripleO can use Puppet now for the configuration of the Overcloud nodes, ask Dan Prince :) Actually usage of Puppet is nowdays the preferred approach and many features are available …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yes, EnablePacemaker! A little old fashioned and not very well suited for OpenStack you say heh? Let's try to talk a bit about this then. News is that we're adding into TripleO/Puppet an option to deploy Pacemaker on the Overcloud nodes. Puppet you said? Sure TripleO can use Puppet now for the configuration of the Overcloud nodes, ask Dan Prince :) Actually usage of Puppet is nowdays the preferred approach and many features are available only in the Puppet scenario ... amongst which the EnablePacemaker bool in subject.&lt;/p&gt;
&lt;p&gt;To go back to Pacemaker, no this won't be the Active/Passive implementation described in the OpenStack HA docs. Not at all, there are no special resource agents for the OpenStack services and more importantly, there is no Active/Passive, when not needed. Instead, the architecture looks a lot more like the Active/Active scenario described in tha doc, except there is Pacemaker coping with a few uncovered bits.&lt;/p&gt;
&lt;p&gt;Whoever tried to setup a RabbitMQ cluster from scratch&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;dan prince&lt;/li&gt;
&lt;li&gt;ha docs&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;####### OLD STUFF #######&lt;/p&gt;
&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite the OpenStack services being designed around a scale-out concept, while attempting to achieve that in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we spotted a number of weak angles, some of which could be worked around, others instead still need some changes in the core service. You're encouraged to try what we have available today and help with the rest.&lt;/p&gt;
&lt;p&gt;So to try out OpenStack HA with &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; you just set a number &amp;gt;= 3 for &lt;code&gt;OVERCLOUD_CONTROLSCALE&lt;/code&gt; and continue with &lt;a class="reference external" href="http://docs.openstack.org/developer/tripleo-incubator/devtest.html"&gt;devtest&lt;/a&gt; as usual. Nodes will be configured appropriately:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export OVERCLOUD_CONTROLSCALE=3
source scripts/devtest_variables.sh
...
&lt;/pre&gt;
&lt;p&gt;Don't forget this is only tested on a few distros for now, I'd pick some Fedora 20.&lt;/p&gt;
&lt;p&gt;On the controller nodes, MariaDB with Galera (for Fedora) is going to provide for a reliable SQL. There is still some work in progress to make sure the Galera cluster can be restarted correctly should all the controllers go down at the same time but, for single node failures, this should be safe to use.&lt;/p&gt;
&lt;p&gt;RabbitMQ nodes are clustered and balanced (via HAProxy), queues replicated.&lt;/p&gt;
&lt;p&gt;And with regards to the OpenStack services, these are configured in a balancing manner (again, using HAProxy) except for those cases where this wouldn't have worked, notably the Neutron L3 agent and the Ceilometer Central agent, yet these are under control via Pacemaker and a single instance is expected to be running at all times. Cinder instead remains uncovered as volumes would require a shared storage for proper HA. A spec has been proposed for this though.&lt;/p&gt;
&lt;p&gt;Also, behind the scenes, the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; template language addon shipped as &lt;em&gt;merge.py&lt;/em&gt; and included in &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates"&gt;tripleo-heat-templates&lt;/a&gt;, which allows for example for scaling of the resources definition, is currently going to be removed and replaced with code living entirely in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And there is more so once you tried, join us on #tripleo &amp;#64; freenode for the real fun!&lt;/p&gt;
</content><category term="openstack"></category><category term="tripleo"></category><category term="high availability"></category><category term="pacemaker"></category><category term="fedoraplanet"></category></entry><entry><title>Ceph for Cinder in TripleO</title><link href="http://giuliofidente.com/2015/01/ceph-for-cinder-in-tripleo.html" rel="alternate"></link><published>2015-01-06T17:15:00+01:00</published><updated>2015-01-06T17:15:00+01:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2015-01-06:/2015/01/ceph-for-cinder-in-tripleo.html</id><summary type="html">&lt;p&gt;A wrap up on the status of TripleO's Cinder HA spec. First, a link to the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, where you can find even more links, to the actual spec (under review) and the code changes (again, still under review). Intent of the blueprint is for the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; deployments to keep Cinder volumes available and Cinder operational in case of failures of any node.&lt;/p&gt;
&lt;p&gt;This said, should $subject sound interesting to you, beware the code still …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A wrap up on the status of TripleO's Cinder HA spec. First, a link to the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, where you can find even more links, to the actual spec (under review) and the code changes (again, still under review). Intent of the blueprint is for the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; deployments to keep Cinder volumes available and Cinder operational in case of failures of any node.&lt;/p&gt;
&lt;p&gt;This said, should $subject sound interesting to you, beware the code still needs reviews, probably polishing and surely more features ... so take this post as a call for help as well! Now some details.&lt;/p&gt;
&lt;p&gt;For Cinder to remain operational, in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we deploy the &lt;code&gt;cinder-{api,schduler,volume}&lt;/code&gt; services on all controller nodes (as it was even before the proposed spec) but we also customize the &lt;code&gt;host&lt;/code&gt; setting so that all instances of &lt;code&gt;cinder-volume&lt;/code&gt; will share an identical identifier. This is so that all instances can perform operations on all volumes, otherwise, the particular host which created a volume would be the only one capable of performing further operations on it. This exposed some potential issues with Cinder due to concurrent mutation of same resource from multiple nodes and they already are taken care of, in Cinder, with the &lt;a class="reference external" href="https://blueprints.launchpad.net/cinder/+spec/cinder-state-enforcer"&gt;state-enforcer blueprint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then, for the volumes to remain available, &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; will deploy a Ceph cluster together with the OpenStack components, to be used as a backend for Cinder. Each and every controller will host a Ceph monitor while an arbitrary number of additional nodes can be configured as Ceph OSDs. The basic idea is that by doing so people will be able to scale out the number of controllers and/or data nodes independently, in an attempt to suit the needs for more space or more CPU resources depending on the use case. In addition to that, network partitioning should also be easy to achieve as traffic from Nova nodes will be directed to the OSDs nodes only for volumes I/O, not to the controllers. Note though that the existing implementation &lt;strong&gt;does not yet&lt;/strong&gt; implement support for the network partitioning but, on a good note, &lt;strong&gt;it does&lt;/strong&gt; use cephx already to secure access to data.&lt;/p&gt;
&lt;p&gt;As of today, assuming you checkout by yourself, as linked from the &lt;a class="reference external" href="https://blueprints.launchpad.net/tripleo/+spec/tripleo-kilo-cinder-ha"&gt;cinder-ha blueprint&lt;/a&gt;, the needed &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; changes, everything should &lt;em&gt;just work&lt;/em&gt; by setting some value &amp;gt; 0 for the &lt;code&gt;CEPHSTORAGESCALE&lt;/code&gt; env variable, which represents the number of OSD nodes you want to deploy. Should you need it, in the Heat templates it is possible to customize a number of useful Ceph settings, like the number of replicas for the data (needs to be lower than the number of OSD nodes). Now go try it out and come back with some feedback! :)&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="tripleo"></category><category term="ceph"></category><category term="high availability"></category><category term="fedoraplanet"></category></entry><entry><title>TripleO vs OpenStack HA</title><link href="http://giuliofidente.com/2014/08/tripleo-vs-openstack-ha.html" rel="alternate"></link><published>2014-08-21T15:02:00+02:00</published><updated>2014-08-21T15:02:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2014-08-21:/2014/08/tripleo-vs-openstack-ha.html</id><summary type="html">&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the topics discussed during the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; mid-cycle meetup in RDU was our status in relation to deploying OpenStack in a highly available manner. This had been worked on for some time and recently reached a usable state.&lt;/p&gt;
&lt;p&gt;Majority of complications seem to come from two factors: 1) we need to guarantee availability of external services too, like the database and the message broker, which aren't exactly designed for a scale-out scenario, 2) despite the OpenStack services being designed around a scale-out concept, while attempting to achieve that in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; we spotted a number of weak angles, some of which could be worked around, others instead still need some changes in the core service. You're encouraged to try what we have available today and help with the rest.&lt;/p&gt;
&lt;p&gt;So to try out OpenStack HA with &lt;a class="reference external" href="https://wiki.openstack.org/wiki/TripleO"&gt;TripleO&lt;/a&gt; you just set a number &amp;gt;= 3 for &lt;code&gt;OVERCLOUD_CONTROLSCALE&lt;/code&gt; and continue with &lt;a class="reference external" href="http://docs.openstack.org/developer/tripleo-incubator/devtest.html"&gt;devtest&lt;/a&gt; as usual. Nodes will be configured appropriately:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export OVERCLOUD_CONTROLSCALE=3
source scripts/devtest_variables.sh
...
&lt;/pre&gt;
&lt;p&gt;Don't forget this is only tested on a few distros for now, I'd pick some Fedora 20.&lt;/p&gt;
&lt;p&gt;On the controller nodes, MariaDB with Galera (for Fedora) is going to provide for a reliable SQL. There is still some work in progress to make sure the Galera cluster can be restarted correctly should all the controllers go down at the same time but, for single node failures, this should be safe to use.&lt;/p&gt;
&lt;p&gt;RabbitMQ nodes are clustered and balanced (via HAProxy), queues replicated.&lt;/p&gt;
&lt;p&gt;And with regards to the OpenStack services, these are configured in a balancing manner (again, using HAProxy) except for those cases where this wouldn't have worked, notably the Neutron L3 agent and the Ceilometer Central agent, yet these are under control via Pacemaker and a single instance is expected to be running at all times. Cinder instead remains uncovered as volumes would require a shared storage for proper HA. A spec has been proposed for this though.&lt;/p&gt;
&lt;p&gt;Also, behind the scenes, the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; template language addon shipped as &lt;em&gt;merge.py&lt;/em&gt; and included in &lt;a class="reference external" href="https://github.com/openstack/tripleo-heat-templates"&gt;tripleo-heat-templates&lt;/a&gt;, which allows for example for scaling of the resources definition, is currently going to be removed and replaced with code living entirely in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And there is more so once you tried, join us on #tripleo &amp;#64; freenode for the real fun!&lt;/p&gt;
</content><category term="openstack"></category><category term="tripleo"></category><category term="high availability"></category><category term="meetup"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Glance - Use Swift as backend</title><link href="http://giuliofidente.com/2013/06/openstack-glance-use-swift-as-backend.html" rel="alternate"></link><published>2013-06-27T02:55:00+02:00</published><updated>2013-06-27T02:55:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-27:/2013/06/openstack-glance-use-swift-as-backend.html</id><summary type="html">&lt;p&gt;On &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; again. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; is the component in charge of hosting the images (and image snapshots) to be cloned for the ephemeral instances. Images usually are just some random big files so it makes perfect sense to use &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt; for such an object (a File Object storage)!&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; image node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This said …&lt;/p&gt;</summary><content type="html">&lt;p&gt;On &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; again. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; is the component in charge of hosting the images (and image snapshots) to be cloned for the ephemeral instances. Images usually are just some random big files so it makes perfect sense to use &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt; for such an object (a File Object storage)!&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt; image node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This said, only few changes are needed to swap from local filesystem storage to &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;. Edit the &lt;code&gt;glance-api.conf&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
default_store = swift
swift_store_auth_address = $KEYSTONE_PROTOCOL://$KEYSTONE_HOST:$KEYSTONE_PORT/v2.0/
swift_store_user = $SERVICE_TENANT_NAME:glance
swift_store_key = $SERVICE_PASSWORD
swift_store_create_container_on_put = True
&lt;/pre&gt;
&lt;p&gt;These are probably self-explanatory but I have a few tips to spare! If you decide to go via https for the keystone service, make sure you can validate locally (on &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Glance"&gt;Glance&lt;/a&gt;) the https certificate. If unsure about the values to be used for the $SERVICE_* variables, these are the same set in the same config file in section &lt;code&gt;keystone_authtoken&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (Dec 2013):&lt;/strong&gt; The user you will set as &lt;code&gt;swift_store_user&lt;/code&gt; must have rights to create new containers in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;, to have that you can assign it the &lt;code&gt;ResellerAdmin&lt;/code&gt; role.&lt;/p&gt;
&lt;p&gt;Also, while not needed, you should consider using port 35357 rather than 5000 for the KEYSTONE_PORT as it is the port where administrative commands can be given.&lt;/p&gt;
&lt;p&gt;Short and straight to the point!&lt;/p&gt;
</content><category term="openstack"></category><category term="glance"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Configure multiple backends</title><link href="http://giuliofidente.com/2013/06/openstack-cinder-configure-multiple-backends.html" rel="alternate"></link><published>2013-06-16T16:37:00+02:00</published><updated>2013-06-16T16:37:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-06-16:/2013/06/openstack-cinder-configure-multiple-backends.html</id><summary type="html">&lt;p&gt;Following &lt;a class="reference external" href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html"&gt;my first post of the series&lt;/a&gt; discussing how to scale &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; to multiple nodes, with this I want to approach the configuration and usage of the multibackend feature landed in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; with the Grizzly release.&lt;/p&gt;
&lt;p&gt;This feature allows you to configure a single volume node for use with more than a single backend driver. You can find all about the few configuration bits needed also in the &lt;a class="reference external" href="http://docs.openstack.org/trunk/openstack-block-storage/admin/content/multi_backend.html"&gt;OpenStack block storage documentation&lt;/a&gt;. That makes …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Following &lt;a class="reference external" href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html"&gt;my first post of the series&lt;/a&gt; discussing how to scale &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; to multiple nodes, with this I want to approach the configuration and usage of the multibackend feature landed in &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; with the Grizzly release.&lt;/p&gt;
&lt;p&gt;This feature allows you to configure a single volume node for use with more than a single backend driver. You can find all about the few configuration bits needed also in the &lt;a class="reference external" href="http://docs.openstack.org/trunk/openstack-block-storage/admin/content/multi_backend.html"&gt;OpenStack block storage documentation&lt;/a&gt;. That makes this post somehow redundant but I wanted to keep up with the series and the topic is well worth to be kept also here.&lt;/p&gt;
&lt;p&gt;As usual, some assumptions before we start:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; architecture&lt;/li&gt;
&lt;li&gt;you have already some &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; volume node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assuming we want our node, configured with some LVM based and an additional NFS based backend, this is what we would need to add into &lt;code&gt;cinder.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
enabled_backends=lvm1,nfs1
[lvm1]
volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver
volume_backend_name=LVM_iSCSI
[nfs1]
nfs_shares_config=${PATH_TO_YOUR_SHARES_FILE}
volume_driver=cinder.volume.drivers.nfs.NfsDriver
volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;enabled_backends&lt;/code&gt; value defines some names (separated by a comma) for the config groups. These do not have to match the driver name nor the backend name.&lt;/p&gt;
&lt;p&gt;When the configuration is complete, to use a particular backend when allocating new volumes, you'll have to pass a &lt;code&gt;volume_type&lt;/code&gt; parameter to the creation command. Such a type has to be created beforehand and to have some backends assigned to it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder type-create lvm
# cinder type-key lvm set volume_backend_name=LVM_iSCSI
# cinder type-create nfs
# cinder type-key nfs set volume_backend_name=NFS
&lt;/pre&gt;
&lt;p&gt;Finally, to create your volumes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder create --volume_type lvm --display_name inlvm 1
&lt;/pre&gt;
&lt;p&gt;For people using the REST interface, to set any &lt;code&gt;type-key&lt;/code&gt; property, including &lt;code&gt;volume_backend_name&lt;/code&gt;, you pass that information along with the request as &lt;a class="reference external" href="https://github.com/openstack/cinder/blob/master/cinder/api/contrib/types_extra_specs.py"&gt;extra specs&lt;/a&gt;. You can list those indeed to make sure the configuration is working as expected:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
#  cinder extra-specs-list
&lt;/pre&gt;
&lt;p&gt;Note that you can have backends of the same type (driver) using different names (say two LVM based backends allocating volumes in different volume groups) or you can also have backends of the same type using the same name! The scheduler is in charge of making the proper decision on how to pickup the correct backend at creation time so a few notes on the filter scheduler (enabled by default in Grizzly):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;firstly it filters the available backends (AvailabilityZoneFilter, CapacityFilter and CapabilitiesFilter are enabled by default and the backend name is matched against the capabilities)&lt;/li&gt;
&lt;li&gt;secondly weights the previously filtered backends (CapacityWeigher is the only one enabled by default)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The CapacityWeigher attributes high score to backends with the most available space, so new volumes are allocated within the backend with the more space available matching the particular name in the request.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE (Nov 2013):&lt;/strong&gt; As reported by Yogev &lt;a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1031010"&gt;in this bug&lt;/a&gt;, misplacing the settings can have dangerous side effects. All settings below the &lt;code&gt;enabled_backends&lt;/code&gt; parameter are actually in some section (eg. [lvm1]) of the ini file rather than [DEFAULT]. Make sure to move the [lvm1] and [nfs1] settings to the bottom of the file and so that all other settings are in the [DEFAULT] section.&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>OpenStack Cinder - Add more volume nodes</title><link href="http://giuliofidente.com/2013/04/openstack-cinder-add-more-volume-nodes.html" rel="alternate"></link><published>2013-04-30T02:00:00+02:00</published><updated>2013-04-30T02:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-30:/2013/04/openstack-cinder-add-more-volume-nodes.html</id><summary type="html">&lt;p&gt;With this being the first of a short series, I'd like to publish some articles intendend to cover the required steps to configure &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; (&lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; block storage service) in a mid/large deployment scenario. The idea is to discuss at least three topics: how to scale the service by adding more volume nodes; how to ensure high-availablity for the API and Scheduler sub-services; leverage the multi-backend feature landed in Grizzly.&lt;/p&gt;
&lt;p&gt;I'm starting with this post …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With this being the first of a short series, I'd like to publish some articles intendend to cover the required steps to configure &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; (&lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; block storage service) in a mid/large deployment scenario. The idea is to discuss at least three topics: how to scale the service by adding more volume nodes; how to ensure high-availablity for the API and Scheduler sub-services; leverage the multi-backend feature landed in Grizzly.&lt;/p&gt;
&lt;p&gt;I'm starting with this post on the scaling issue first. &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; is composed of three main parts, the API server, the scheduler and the volume service. The volume service is some sort of abstraction layer between the API and the actual resources provider.&lt;/p&gt;
&lt;p&gt;By adding more volume nodes into the environment you will be able to increase the total offering of block storage to the tenants. Each volume node can either provide volumes by allocating them locally or on a remote container like an NFS or GlusterFS share.&lt;/p&gt;
&lt;p&gt;Some assumptions before getting into the practice:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;you're familiar with the general OpenStack architecture&lt;/li&gt;
&lt;li&gt;you have at least one Cinder node configured and working as expected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to do on the candidate node is to install the required packages. I'm running the examples on CentOS and using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository which makes this step as simple as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-cinder
&lt;/pre&gt;
&lt;p&gt;If you plan to host new volumes using the locally available storage dont' forget to create a volume group called &lt;code&gt;cinder-volumes&lt;/code&gt; (the name can be configured via the &lt;code&gt;cinder_volume&lt;/code&gt; parameter). Also don't forget to configure the &lt;code&gt;tgtd&lt;/code&gt; to include the config files created dynamically by &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt;. Add a line like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
include /etc/cinder/volumes/*
&lt;/pre&gt;
&lt;p&gt;in your &lt;code&gt;/etc/tgt/targets.conf&lt;/code&gt; file. Now enable and start the &lt;code&gt;tgtd&lt;/code&gt; service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# chkconfig tgtd on
# service tgtd start
&lt;/pre&gt;
&lt;p&gt;Amongst the three init services installed by &lt;code&gt;openstack-cinder&lt;/code&gt; you only need to run &lt;code&gt;openstack-cinder-volume&lt;/code&gt;, which gets configured in &lt;code&gt;/etc/cinder/cinder.conf&lt;/code&gt;. Configure it to connect to the existing &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; database (the db in use by the pre-existing node) and to the existing AMQP broker (again, in use by the pre-existing node) by setting the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection=mysql://cinder:${CINDER_DB_PASSWORD}&amp;#64;${CINDER_DB_HOST}/cinder
qpid_hostname=${QPIDD_BROKER}
&lt;/pre&gt;
&lt;p&gt;Set the credentials if needed and/or change the &lt;code&gt;rpc_backend&lt;/code&gt; setting if you're not using &lt;a class="reference external" href="http://qpid.apache.org/"&gt;Qpid&lt;/a&gt; as your message broker. One more setting, not really required to change but worth checking if you're using the local resources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
iscsi_ip_address=${TGTD_IP_ADDRESS}
&lt;/pre&gt;
&lt;p&gt;That should match the public ip address of the volume node just installed. The iSCSI targets created locally using &lt;code&gt;tgtadm/tgtd&lt;/code&gt; have to be reachable by the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Nova"&gt;Nova&lt;/a&gt; nodes. The IP address of each target is stored in the database with every volume created. The &lt;code&gt;iscsi_ip_address&lt;/code&gt; prameter sets what is the IP address to be given to the initiators.&lt;/p&gt;
&lt;p&gt;At this point you should be ready to start the volume service:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# service openstack-cinder-volume start
&lt;/pre&gt;
&lt;p&gt;Verify that it started by checking the logs (&lt;code&gt;/var/log/cinder/volume.log&lt;/code&gt;) or by issueing on any &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; node:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cinder-manage host list
&lt;/pre&gt;
&lt;p&gt;you should see all of your volume nodes listed. From now on you can create new volumes as usual and they will be allocated on any of the volume nodes, keep in mind that the scheduler will default to the node with the most space available.&lt;/p&gt;
</content><category term="openstack"></category><category term="cinder"></category><category term="fedoraplanet"></category></entry><entry><title>Deploy OpenStack Heat on RHEL (and derivates)</title><link href="http://giuliofidente.com/2013/04/deploy-openstack-heat-on-rhel-and-derivates.html" rel="alternate"></link><published>2013-04-16T11:00:00+02:00</published><updated>2013-04-16T11:00:00+02:00</updated><author><name>Giulio Fidente</name></author><id>tag:giuliofidente.com,2013-04-16:/2013/04/deploy-openstack-heat-on-rhel-and-derivates.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE (June 2013):&lt;/strong&gt; this post has been &lt;a class="reference external" href="http://openstack.redhat.com/Deploy_Heat_and_launch_your_first_Application"&gt;published on the RDO site&lt;/a&gt; and is now maintained there.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; provides orchestration of composite cloud applications using the CloudFormation API and templates; it is an incubated project of OpenStack. Its development cycle is to be Integrated in Havana and follow the full OpenStack release process. I want to go trough the steps needed to install and configure it as the &lt;a class="reference external" href="http://docs.openstack.org"&gt;official documentation&lt;/a&gt; is still scarce on the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;UPDATE (June 2013):&lt;/strong&gt; this post has been &lt;a class="reference external" href="http://openstack.redhat.com/Deploy_Heat_and_launch_your_first_Application"&gt;published on the RDO site&lt;/a&gt; and is now maintained there.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; provides orchestration of composite cloud applications using the CloudFormation API and templates; it is an incubated project of OpenStack. Its development cycle is to be Integrated in Havana and follow the full OpenStack release process. I want to go trough the steps needed to install and configure it as the &lt;a class="reference external" href="http://docs.openstack.org"&gt;official documentation&lt;/a&gt; is still scarce on the matter. Firstly, what it does?&lt;/p&gt;
&lt;blockquote&gt;
Heat is a service to orchestrate multiple composite cloud applications using the AWS CloudFormation template format, through both an OpenStack-native ReST API and a CloudFormation-compatible Query API.&lt;/blockquote&gt;
&lt;p&gt;So you're going to deploy a composite application (made up of more than a single instance) on the cloud infrastructure, this also involves launchtime customizations of the VMs but, before start, some assumptions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;I'm using CentOS 6.4 / MySQL&lt;/li&gt;
&lt;li&gt;I'm using the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; repository to install the packages&lt;/li&gt;
&lt;li&gt;The core &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; infrastructure is already configured and in good shape&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you don't have a working &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; deployment yet, I recommend you to follow the instructions on the &lt;a class="reference external" href="http://openstack.redhat.com"&gt;RDO&lt;/a&gt; site, you'll get one up and running in minutes by using &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;. When that is finished, start by installing the required packages for &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; to work:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum install openstack-heat-*
&lt;/pre&gt;
&lt;p&gt;You'll get four new services installed: an engine, a native api, a cloudformation compatible api, a cloudwatch compatible api. You don't have to deploy them all on a single host but for the purpose of this guide it will be fine to do so.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="configuration"&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; comes with a script which creates (and populates) the needed database for it to work but you need to know your MySQL's &lt;code&gt;root&lt;/code&gt; account password. If you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;, than that is saved as &lt;code&gt;CONFIG_MYSQL_PW&lt;/code&gt; in the answers file (&lt;code&gt;/root/packstack-answers*&lt;/code&gt; by default). Now run the prepare script:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-db-setup rpm -y -r ${MYSQL_ROOT_PASSWORD} -p ${HEAT_DB_PASSWORD_OF_CHOICE}
&lt;/pre&gt;
&lt;p&gt;Check in &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; that your database connection string is correct:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sql_connection = mysql://heat:${HEAT_DB_PASSWORD}&amp;#64;localhost/heat
&lt;/pre&gt;
&lt;p&gt;Now go trough the &lt;em&gt;usual&lt;/em&gt; steps needed to create a new user, service and endpoint with Keystone and don't forget to source the admin credentials before starting (which are in &lt;code&gt;/root/keystonerc_admin&lt;/code&gt; if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone user-create --name heat --pass ${HEAT_USER_PASSWORD_OF_CHOICE}
# keystone user-role-add --user heat --role admin --tenant ${SERVICES_TENANT_NAME}
# keystone service-create --name heat --type orchestration
# keystone service-create --name heat-cfn --type cloudformation
# keystone endpoint-create --region RegionOne --service-id ${HEAT_CFN_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --adminurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot; --internalurl &amp;quot;http://${HEAT_CFN_HOSTNAME}:8000/v1&amp;quot;
# keystone endpoint-create --region RegionOne --service-id ${HEAT_SERVICE_ID} --publicurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot; --adminurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s --internalurl &amp;quot;http://${HEAT_HOSTNAME}:8004/v1/%(tenant_id)s&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Update the paste files at &lt;code&gt;/etc/heat/heat-api{,-cfn,-cloudwatch}-paste.ini&lt;/code&gt; with the credentials just created:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
admin_tenant_name = ${SERVICES_TENANT_NAME}
admin_user = heat
admin_password = ${HEAT_USER_PASSWORD}
&lt;/pre&gt;
&lt;p&gt;In there you also need to make sure that the following variables are pointing to your Keystone host (127.0.0.1 should just work if you've used &lt;a class="reference external" href="http://wiki.openstack.org/Packstack"&gt;PackStack&lt;/a&gt; as Keystone is probably installed on the same host):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
service_host = ${KEYSTONE_HOSTNAME}
auth_host = ${KEYSTONE_HOSTNAME}
auth_uri = http://${KEYSTONE_HOSTNAME}:35357/v2.0
keystone_ec2_uri = http://${KEYSTONE_HOSTNAME}:5000/v2.0/ec2tokens
&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;/etc/heat/heat-engine.conf&lt;/code&gt; you've to make instead sure that the following variables &lt;strong&gt;do not&lt;/strong&gt; point to 127.0.0.1 even though the services are actually hosted on the same system because URLs will be passed over to the VMs, which don't have them available locally:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
heat_metadata_server_url = http://${HEAT_CFN_HOSTNAME}:8000
heat_waitcondition_server_url = http://${HEAT_CFN_HOSTNAME}:8000/v1/waitcondition
heat_watch_server_url = http://${HEAT_CLOUDWATCH_HOSTNAME}:8003
&lt;/pre&gt;
&lt;p&gt;The application templates can use wait conditions and signaling for the orchestration, &lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; needs to create special users to receive the progress data and these users are, by default, given the role of &lt;code&gt;heat_stack_user&lt;/code&gt;. You can configure the role name in &lt;code&gt;heat-engine.conf&lt;/code&gt; or just create a so called role:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# keystone role-create --name heat_stack_user
&lt;/pre&gt;
&lt;p&gt;The configuration should now be complete and the services can be started:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# cd /etc/init.d &amp;amp;&amp;amp; for s in $(ls openstack-heat-*); do chkconfig $s on &amp;amp;&amp;amp; service $s start; done
&lt;/pre&gt;
&lt;p&gt;Make sure by checking the logs that everything was started successfully. Specifically, in case the engine service reports &lt;code&gt;ImportError: cannot import name Random&lt;/code&gt; then you're probably using an old version of &lt;code&gt;pycrypto&lt;/code&gt;. A fix has been merged upstream to workaround the issue. It's &lt;a class="reference external" href="https://review.openstack.org/#/c/26759/"&gt;a trivial change&lt;/a&gt; which you can apply manually to &lt;code&gt;heat/common/crypt.py&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="get-the-demo-files"&gt;
&lt;h2&gt;Get the demo files&lt;/h2&gt;
&lt;p&gt;It is time now to launch your first multi-instance cloud application! There are a number of sample templates available in the &lt;a class="reference external" href="https://github.com/openstack/heat"&gt;github repo&lt;/a&gt;, download the composed Wordpress example with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# wget https://raw.github.com/openstack/heat-templates/master/cfn/WordPress_Composed_Instances.template
&lt;/pre&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.openstack.org/wiki/Heat"&gt;Heat&lt;/a&gt; can use the templates distributed for &lt;a class="reference external" href="http://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt;. These expect you to have a well known set of flavor types defined while the default flavors available in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; don't match strictly such a collection. To avoid the need of hack the templates, you can use an helpful script which recreates in &lt;a class="reference external" href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; the same flavors from AWS:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# curl https://raw.github.com/openstack/heat/master/tools/nova_create_flavors.sh | bash
&lt;/pre&gt;
&lt;p&gt;Every template also provides you with a list of usable distros and map these into an AMI string, for each arch. You will have to populate Glance with an image matching the AMI string that the template file is expecting to find.&lt;/p&gt;
&lt;p&gt;There is a tool, called &lt;a class="reference external" href="https://github.com/sdake/heat-jeos"&gt;heat-jeos&lt;/a&gt;, which can be used to create the JEOS images and upload them to Glance but there is also a collection of prebuilt images at: &lt;a class="reference external" href="http://fedorapeople.org/groups/heat/prebuilt-jeos-images/"&gt;http://fedorapeople.org/groups/heat/prebuilt-jeos-images/&lt;/a&gt; so I suggest you to just download one from &lt;code&gt;F17-x86_64-cfntools.qcow2&lt;/code&gt; or &lt;code&gt;U10-x86_64-cfntools.qcow2&lt;/code&gt; (which are referred by many if not all the templates available in the Heat's repo). To upload the F17 x86_64 image in Glance:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# glance image-create --name F17-x86_64-cfntools --disk-format qcow2 --container-format bare --is-public True --copy-from http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2
&lt;/pre&gt;
&lt;p&gt;While that is downloading, create a new keypair or upload you public key in nova to make sure you'll be able to login on the VMs using SSH:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# nova keypair-add --pub_key ~/.ssh/id_rsa.pub userkey
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="launch"&gt;
&lt;h2&gt;Launch!&lt;/h2&gt;
&lt;p&gt;It is time for the real fun now, launch your first composed application with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn create wordpress --template-file=WordPress_Composed_Instances.template --parameters=&amp;quot;DBUsername=wp;DBPassword=wp;KeyName=userkey;LinuxDistribution=F17&amp;quot;
&lt;/pre&gt;
&lt;p&gt;More parameters could have passed, note for instance the LinuxDistribution parameter discussed above. Now the interesting stuff:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn list
# heat-cfn event-list wordpress
&lt;/pre&gt;
&lt;p&gt;After the VMs are launched, the mysql/httpd/wordpress installation and configuration begins, the process is driven by the &lt;code&gt;cfntools&lt;/code&gt;, installed in the VMs images. It will take quite some time, despite the &lt;code&gt;event-list&lt;/code&gt; reporting completion for the WordPress install too early (there is signaling, via &lt;code&gt;cfn-signal&lt;/code&gt;, only in the MySQL template). You can login on the instances and check the logs or just use &lt;code&gt;ps&lt;/code&gt; to see how things are going. After some minutes the setup should be finished:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# heat-cfn describe wordpress
# wget ${WebsiteURL} // that is an URL from the previous command!
&lt;/pre&gt;
&lt;p&gt;If anything goes wrong, check the logs at &lt;code&gt;/var/log/heat/engine.log&lt;/code&gt; or look at the scripts passed as &lt;code&gt;UserData&lt;/code&gt; to the instances, these should be found in &lt;code&gt;/var/lib/cloud/data/&lt;/code&gt;. Time to hack your very own template and delete the test deployment! :)&lt;/p&gt;
&lt;/div&gt;
</content><category term="openstack"></category><category term="heat"></category><category term="fedoraplanet"></category><category term="rhel"></category><category term="centos"></category></entry></feed>